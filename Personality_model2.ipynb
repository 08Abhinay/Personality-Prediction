{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WrmxxI1-8vYS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Bidirectional, Dropout, GlobalMaxPool1D, Concatenate\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, TimeDistributed, Attention\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "tuZNQIe7BY61"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "mbti_dataset = pd.read_csv(\"mbti_1.csv\", encoding='ISO-8859-1')\n",
        "big5_dataset = pd.read_csv(\"big5_1.csv\", encoding='ISO-8859-1')"
      ],
      "metadata": {
        "id": "I_kEMNGu8_Mk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename and select relevant columns\n",
        "mbti_dataset.rename(columns={'type': 'MBTI', 'posts': 'TEXT'}, inplace=True)\n",
        "big5_columns_to_keep = ['TEXT', 'cEXT', 'cNEU', 'cAGR', 'cCON', 'cOPN']\n",
        "big5_dataset = big5_dataset[big5_columns_to_keep]"
      ],
      "metadata": {
        "id": "XRIm3DyB9BOR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert Big5 traits to MBTI type\n",
        "def big5_to_mbti(row):\n",
        "    mbti = ''\n",
        "    mbti += 'E' if row['cEXT'] == 'y' else 'I'\n",
        "    mbti += 'N' if row['cOPN'] == 'y' else 'S'\n",
        "    mbti += 'F' if row['cAGR'] == 'y' else 'T'\n",
        "    mbti += 'J' if row['cCON'] == 'y' else 'P'\n",
        "    return mbti"
      ],
      "metadata": {
        "id": "D_sk9aX49U1i"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply conversion\n",
        "big5_dataset['MBTI'] = big5_dataset.apply(big5_to_mbti, axis=1)\n",
        "full_dataset = pd.concat([mbti_dataset[['TEXT', 'MBTI']], big5_dataset[['TEXT', 'MBTI']]], ignore_index=True)\n"
      ],
      "metadata": {
        "id": "vD2nJjX19Y39"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess text data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9Dp1e119gcV",
        "outputId": "65796a70-3cec-4cdb-c0fc-3bd5896ae216"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Lowercase text\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    return \" \".join(filtered_words)"
      ],
      "metadata": {
        "id": "pda3zwWd9apR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWO8ReSX_qcv",
        "outputId": "16e99bfa-f115-48ff-c8b0-38371b2d24d8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_dataset['TEXT'] = full_dataset['TEXT'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "0UgMhqvH9dQQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(full_dataset['TEXT'], full_dataset['MBTI'], test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "U8Ry7nMQ_j5m"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize text\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Padding sequences\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=100, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=100, padding='post')\n"
      ],
      "metadata": {
        "id": "0bnVRuS__w0x"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fY1-8yy0AByD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Me5Yj0rzADIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "num_classes = np.max(y_train_encoded) + 1\n",
        "y_train_cat = to_categorical(y_train_encoded, num_classes)\n",
        "y_test_cat = to_categorical(y_test_encoded, num_classes)"
      ],
      "metadata": {
        "id": "0juGVgVnAEAV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "# Checkpoint callback\n",
        "checkpoint_path = \"/content/drive/MyDrive/Model_data/model_checkpoint_personality_pred.h5\"\n",
        "checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n"
      ],
      "metadata": {
        "id": "lj_flYuoAJ8D"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "embedding_dim = 64\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "D-ITxEJ-BJW8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import GlobalAveragePooling1D\n",
        "input_text = Input(shape=(100,))\n",
        "embedding_layer = Embedding(vocab_size, embedding_dim, input_length=100)(input_text)\n",
        "bi_lstm = Bidirectional(LSTM(64, return_sequences=True))(embedding_layer)\n",
        "attention = Attention()([bi_lstm, bi_lstm])\n",
        "pooling_layer = GlobalAveragePooling1D()(attention)  # This collapses the sequence dimension\n",
        "dense_layer = Dense(64, activation='relu')(pooling_layer)\n",
        "output_layer = Dense(num_classes, activation='softmax')(dense_layer)"
      ],
      "metadata": {
        "id": "RZJxXXPgBPgI"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(inputs=input_text, outputs=output_layer)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "zjpGtcVXBR7c"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(X_train_pad, y_train_cat, epochs=50, batch_size=64, validation_data=(X_test_pad, y_test_cat))\n"
      ],
      "metadata": {
        "id": "gNXRw2HVBvqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Layer\n",
        "import keras.backend as K\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "        self.W = self.add_weight(name='att_weight',\n",
        "                                 shape=(input_shape[-1], 1),\n",
        "                                 initializer='random_normal',\n",
        "                                 trainable=True)\n",
        "        self.b = self.add_weight(name='att_bias',\n",
        "                                 shape=(input_shape[1], 1),\n",
        "                                 initializer='zeros',\n",
        "                                 trainable=True)\n",
        "        super(AttentionLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        e = K.tanh(K.dot(inputs, self.W) + self.b)\n",
        "        e = K.squeeze(e, axis=-1)\n",
        "        alpha = K.softmax(e)\n",
        "        context = inputs * K.expand_dims(alpha, -1)\n",
        "        context = K.sum(context, axis=1)\n",
        "        return context\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[-1])\n",
        "\n",
        "# Use this attention in your model\n",
        "attention = AttentionLayer()(bi_lstm)\n"
      ],
      "metadata": {
        "id": "QQVFZtzBB7jx"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import GlobalAveragePooling1D\n",
        "input_text = Input(shape=(100,))\n",
        "embedding_layer = Embedding(vocab_size, embedding_dim, input_length=100)(input_text)\n",
        "bi_lstm = Bidirectional(LSTM(64, return_sequences=True))(embedding_layer)\n",
        "attention = Attention()([bi_lstm, bi_lstm])\n",
        "pooling_layer = GlobalAveragePooling1D()(attention)  # This collapses the sequence dimension\n",
        "dense_layer = Dense(64, activation='relu')(pooling_layer)\n",
        "output_layer = Dense(num_classes, activation='softmax')(dense_layer)"
      ],
      "metadata": {
        "id": "Qa9-sgtnDaYZ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(inputs=input_text, outputs=output_layer)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "UpAYhjclDdzu"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(X_train_pad, y_train_cat, epochs=50, batch_size=64, validation_data=(X_test_pad, y_test_cat))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOn3paY1Dg3i",
        "outputId": "811d7df4-a810-4179-c5d8-609075442149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "140/140 [==============================] - 20s 100ms/step - loss: 2.4706 - accuracy: 0.1764 - val_loss: 2.3575 - val_accuracy: 0.1907\n",
            "Epoch 2/50\n",
            "140/140 [==============================] - 8s 56ms/step - loss: 2.3163 - accuracy: 0.1917 - val_loss: 2.3857 - val_accuracy: 0.1768\n",
            "Epoch 3/50\n",
            "140/140 [==============================] - 5s 32ms/step - loss: 2.1659 - accuracy: 0.2294 - val_loss: 2.3332 - val_accuracy: 0.2104\n",
            "Epoch 4/50\n",
            "140/140 [==============================] - 4s 28ms/step - loss: 1.9861 - accuracy: 0.2862 - val_loss: 2.4135 - val_accuracy: 0.2279\n",
            "Epoch 5/50\n",
            "140/140 [==============================] - 4s 31ms/step - loss: 1.7836 - accuracy: 0.3520 - val_loss: 2.5760 - val_accuracy: 0.2373\n",
            "Epoch 6/50\n",
            "140/140 [==============================] - 3s 21ms/step - loss: 1.5864 - accuracy: 0.4227 - val_loss: 2.7730 - val_accuracy: 0.2176\n",
            "Epoch 7/50\n",
            "140/140 [==============================] - 3s 21ms/step - loss: 1.4193 - accuracy: 0.4925 - val_loss: 3.0739 - val_accuracy: 0.2207\n",
            "Epoch 8/50\n",
            "140/140 [==============================] - 3s 24ms/step - loss: 1.2533 - accuracy: 0.5524 - val_loss: 3.3365 - val_accuracy: 0.2037\n",
            "Epoch 9/50\n",
            "140/140 [==============================] - 4s 30ms/step - loss: 1.1160 - accuracy: 0.6158 - val_loss: 3.5760 - val_accuracy: 0.2055\n",
            "Epoch 10/50\n",
            "140/140 [==============================] - 3s 20ms/step - loss: 0.9617 - accuracy: 0.6732 - val_loss: 4.1756 - val_accuracy: 0.2037\n",
            "Epoch 11/50\n",
            "140/140 [==============================] - 3s 22ms/step - loss: 0.8153 - accuracy: 0.7284 - val_loss: 4.6776 - val_accuracy: 0.2162\n",
            "Epoch 12/50\n",
            "140/140 [==============================] - 3s 22ms/step - loss: 0.7258 - accuracy: 0.7565 - val_loss: 4.9220 - val_accuracy: 0.2055\n",
            "Epoch 13/50\n",
            "140/140 [==============================] - 4s 29ms/step - loss: 0.6168 - accuracy: 0.7986 - val_loss: 5.1932 - val_accuracy: 0.2046\n",
            "Epoch 14/50\n",
            "140/140 [==============================] - 3s 22ms/step - loss: 0.5210 - accuracy: 0.8312 - val_loss: 5.7913 - val_accuracy: 0.2104\n",
            "Epoch 15/50\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 0.4611 - accuracy: 0.8477 - val_loss: 6.0622 - val_accuracy: 0.2095\n",
            "Epoch 16/50\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 0.3951 - accuracy: 0.8731 - val_loss: 6.5626 - val_accuracy: 0.2113\n",
            "Epoch 17/50\n",
            "140/140 [==============================] - 3s 18ms/step - loss: 0.3370 - accuracy: 0.8912 - val_loss: 6.8665 - val_accuracy: 0.1992\n",
            "Epoch 18/50\n",
            "140/140 [==============================] - 4s 26ms/step - loss: 0.2832 - accuracy: 0.9072 - val_loss: 7.3031 - val_accuracy: 0.1965\n",
            "Epoch 19/50\n",
            "140/140 [==============================] - 3s 21ms/step - loss: 0.2466 - accuracy: 0.9225 - val_loss: 7.5371 - val_accuracy: 0.2059\n",
            "Epoch 20/50\n",
            "140/140 [==============================] - 3s 20ms/step - loss: 0.2258 - accuracy: 0.9282 - val_loss: 7.9621 - val_accuracy: 0.1916\n",
            "Epoch 21/50\n",
            "140/140 [==============================] - 2s 18ms/step - loss: 0.1857 - accuracy: 0.9437 - val_loss: 8.8946 - val_accuracy: 0.1835\n",
            "Epoch 22/50\n",
            "140/140 [==============================] - 2s 17ms/step - loss: 0.1834 - accuracy: 0.9439 - val_loss: 8.5889 - val_accuracy: 0.1969\n",
            "Epoch 23/50\n",
            "140/140 [==============================] - 3s 24ms/step - loss: 0.1360 - accuracy: 0.9611 - val_loss: 8.9036 - val_accuracy: 0.2023\n",
            "Epoch 24/50\n",
            "140/140 [==============================] - 3s 21ms/step - loss: 0.1171 - accuracy: 0.9674 - val_loss: 9.3129 - val_accuracy: 0.1925\n",
            "Epoch 25/50\n",
            "140/140 [==============================] - 2s 17ms/step - loss: 0.1277 - accuracy: 0.9638 - val_loss: 9.7235 - val_accuracy: 0.2050\n",
            "Epoch 26/50\n",
            "140/140 [==============================] - 2s 18ms/step - loss: 0.1082 - accuracy: 0.9685 - val_loss: 9.3661 - val_accuracy: 0.1925\n",
            "Epoch 27/50\n",
            "140/140 [==============================] - 4s 27ms/step - loss: 0.1042 - accuracy: 0.9711 - val_loss: 9.0866 - val_accuracy: 0.2059\n",
            "Epoch 28/50\n",
            "140/140 [==============================] - 6s 44ms/step - loss: 0.0805 - accuracy: 0.9786 - val_loss: 9.1825 - val_accuracy: 0.1916\n",
            "Epoch 29/50\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 0.0682 - accuracy: 0.9816 - val_loss: 10.4301 - val_accuracy: 0.1925\n",
            "Epoch 30/50\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 0.0747 - accuracy: 0.9791 - val_loss: 10.2530 - val_accuracy: 0.1956\n",
            "Epoch 31/50\n",
            "140/140 [==============================] - 2s 17ms/step - loss: 0.0708 - accuracy: 0.9807 - val_loss: 10.2438 - val_accuracy: 0.1996\n",
            "Epoch 32/50\n",
            "140/140 [==============================] - 2s 18ms/step - loss: 0.0683 - accuracy: 0.9805 - val_loss: 10.0351 - val_accuracy: 0.2041\n",
            "Epoch 33/50\n",
            "140/140 [==============================] - 3s 24ms/step - loss: 0.0750 - accuracy: 0.9786 - val_loss: 10.5055 - val_accuracy: 0.1947\n",
            "Epoch 34/50\n",
            "140/140 [==============================] - 3s 20ms/step - loss: 0.0436 - accuracy: 0.9892 - val_loss: 11.1376 - val_accuracy: 0.1987\n",
            "Epoch 35/50\n",
            "140/140 [==============================] - 2s 17ms/step - loss: 0.0392 - accuracy: 0.9906 - val_loss: 11.5309 - val_accuracy: 0.1938\n",
            "Epoch 36/50\n",
            "140/140 [==============================] - 2s 17ms/step - loss: 0.0296 - accuracy: 0.9934 - val_loss: 11.6756 - val_accuracy: 0.1974\n",
            "Epoch 37/50\n",
            "140/140 [==============================] - 2s 17ms/step - loss: 0.0292 - accuracy: 0.9933 - val_loss: 11.3233 - val_accuracy: 0.1925\n",
            "Epoch 38/50\n",
            "140/140 [==============================] - 3s 23ms/step - loss: 0.0236 - accuracy: 0.9944 - val_loss: 11.6461 - val_accuracy: 0.2019\n",
            "Epoch 39/50\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 0.0186 - accuracy: 0.9956 - val_loss: 11.7446 - val_accuracy: 0.1978\n",
            "Epoch 40/50\n",
            "140/140 [==============================] - 2s 17ms/step - loss: 0.0283 - accuracy: 0.9925 - val_loss: 11.7133 - val_accuracy: 0.1961\n",
            "Epoch 41/50\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 0.0472 - accuracy: 0.9879 - val_loss: 11.5395 - val_accuracy: 0.2100\n",
            "Epoch 42/50\n",
            "140/140 [==============================] - 2s 17ms/step - loss: 0.0750 - accuracy: 0.9772 - val_loss: 11.2776 - val_accuracy: 0.2032\n",
            "Epoch 43/50\n",
            "140/140 [==============================] - 3s 21ms/step - loss: 0.0825 - accuracy: 0.9763 - val_loss: 11.3498 - val_accuracy: 0.2055\n",
            "Epoch 44/50\n",
            "140/140 [==============================] - 3s 22ms/step - loss: 0.0448 - accuracy: 0.9870 - val_loss: 11.5470 - val_accuracy: 0.2055\n",
            "Epoch 45/50\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 0.0323 - accuracy: 0.9926 - val_loss: 11.8455 - val_accuracy: 0.1983\n",
            "Epoch 46/50\n",
            "140/140 [==============================] - 2s 18ms/step - loss: 0.0199 - accuracy: 0.9957 - val_loss: 12.2003 - val_accuracy: 0.2073\n",
            "Epoch 47/50\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 0.0081 - accuracy: 0.9989 - val_loss: 12.4097 - val_accuracy: 0.2023\n",
            "Epoch 48/50\n",
            "140/140 [==============================] - 3s 21ms/step - loss: 0.0032 - accuracy: 0.9997 - val_loss: 13.2025 - val_accuracy: 0.2014\n",
            "Epoch 49/50\n",
            "140/140 [==============================] - 3s 23ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 13.3401 - val_accuracy: 0.1974\n",
            "Epoch 50/50\n",
            "140/140 [==============================] - 2s 16ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 13.8844 - val_accuracy: 0.2041\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7af73bd68910>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, Dropout\n",
        "from keras.regularizers import l2"
      ],
      "metadata": {
        "id": "aEDZOCtREgM_"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YVuC-ZrXEiDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "embedding_dim = 64\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "input_text = Input(shape=(100,))\n",
        "embedding_layer = Embedding(vocab_size, embedding_dim, input_length=100)(input_text)\n",
        "bi_lstm = Bidirectional(LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01)))(embedding_layer)\n",
        "dropout = Dropout(0.5)(bi_lstm)\n",
        "attention = Attention()([dropout, dropout])\n",
        "dropout_2 = Dropout(0.5)(attention)\n",
        "dense_layer = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(dropout_2)\n",
        "output_layer = Dense(num_classes, activation='softmax')(dense_layer)"
      ],
      "metadata": {
        "id": "H71BAaidDiW7"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dropout, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.layers import Attention\n",
        "\n",
        "# Parameters\n",
        "embedding_dim = 64\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Make sure tokenizer is defined above\n",
        "num_classes = 16  # Change this to the actual number of classes you have\n",
        "\n",
        "# Model architecture\n",
        "input_text = Input(shape=(100,))\n",
        "embedding_layer = Embedding(vocab_size, embedding_dim, input_length=100)(input_text)\n",
        "bi_lstm = Bidirectional(LSTM(64, return_sequences=False, kernel_regularizer=l2(0.01)))(embedding_layer)  # return_sequences is now False\n",
        "dropout = Dropout(0.5)(bi_lstm)\n",
        "dense_layer = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(dropout)\n",
        "output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n",
        "\n",
        "# Create and compile the model\n",
        "model = Model(inputs=input_text, outputs=output_layer)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "eSzqaFfMEvgY"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(X_train_pad, y_train_cat, epochs=50, batch_size=64, validation_data=(X_test_pad, y_test_cat))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CMd1Nw9FqDr",
        "outputId": "78d8d514-f490-4e24-c49b-d0f8700f5065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "140/140 [==============================] - 19s 98ms/step - loss: 3.4682 - accuracy: 0.1710 - val_loss: 2.6515 - val_accuracy: 0.1310\n",
            "Epoch 2/50\n",
            "140/140 [==============================] - 7s 52ms/step - loss: 2.5540 - accuracy: 0.1713 - val_loss: 2.4780 - val_accuracy: 0.1812\n",
            "Epoch 3/50\n",
            "140/140 [==============================] - 4s 30ms/step - loss: 2.4505 - accuracy: 0.1741 - val_loss: 2.4164 - val_accuracy: 0.1929\n",
            "Epoch 4/50\n",
            "140/140 [==============================] - 4s 26ms/step - loss: 2.4073 - accuracy: 0.1861 - val_loss: 2.4016 - val_accuracy: 0.1925\n",
            "Epoch 5/50\n",
            "140/140 [==============================] - 3s 25ms/step - loss: 2.3831 - accuracy: 0.1879 - val_loss: 2.4183 - val_accuracy: 0.1889\n",
            "Epoch 6/50\n",
            "140/140 [==============================] - 4s 27ms/step - loss: 2.3654 - accuracy: 0.1936 - val_loss: 2.4120 - val_accuracy: 0.1866\n",
            "Epoch 7/50\n",
            "140/140 [==============================] - 3s 23ms/step - loss: 2.3496 - accuracy: 0.1934 - val_loss: 2.4069 - val_accuracy: 0.1862\n",
            "Epoch 8/50\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 2.3424 - accuracy: 0.1956 - val_loss: 2.4164 - val_accuracy: 0.1884\n",
            "Epoch 9/50\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 2.3330 - accuracy: 0.1923 - val_loss: 2.4112 - val_accuracy: 0.1835\n",
            "Epoch 10/50\n",
            "140/140 [==============================] - 4s 27ms/step - loss: 2.3077 - accuracy: 0.2006 - val_loss: 2.4147 - val_accuracy: 0.1812\n",
            "Epoch 11/50\n",
            "140/140 [==============================] - 3s 21ms/step - loss: 2.2860 - accuracy: 0.2009 - val_loss: 2.4523 - val_accuracy: 0.1844\n",
            "Epoch 12/50\n",
            "140/140 [==============================] - 3s 21ms/step - loss: 2.2360 - accuracy: 0.2094 - val_loss: 2.4552 - val_accuracy: 0.1763\n",
            "Epoch 13/50\n",
            "140/140 [==============================] - 3s 20ms/step - loss: 2.2034 - accuracy: 0.2160 - val_loss: 2.4672 - val_accuracy: 0.1781\n",
            "Epoch 14/50\n",
            "140/140 [==============================] - 3s 20ms/step - loss: 2.1709 - accuracy: 0.2198 - val_loss: 2.4920 - val_accuracy: 0.1768\n",
            "Epoch 15/50\n",
            "140/140 [==============================] - 4s 25ms/step - loss: 2.1477 - accuracy: 0.2277 - val_loss: 2.5638 - val_accuracy: 0.1786\n",
            "Epoch 16/50\n",
            "140/140 [==============================] - 2s 17ms/step - loss: 2.1263 - accuracy: 0.2292 - val_loss: 2.5829 - val_accuracy: 0.1777\n",
            "Epoch 17/50\n",
            "140/140 [==============================] - 3s 18ms/step - loss: 2.1042 - accuracy: 0.2330 - val_loss: 2.5863 - val_accuracy: 0.1696\n",
            "Epoch 18/50\n",
            "140/140 [==============================] - 2s 17ms/step - loss: 2.0841 - accuracy: 0.2355 - val_loss: 2.5712 - val_accuracy: 0.1768\n",
            "Epoch 19/50\n",
            "140/140 [==============================] - 2s 17ms/step - loss: 2.0653 - accuracy: 0.2352 - val_loss: 2.6999 - val_accuracy: 0.1691\n",
            "Epoch 20/50\n",
            "140/140 [==============================] - 4s 26ms/step - loss: 2.0399 - accuracy: 0.2451 - val_loss: 2.6974 - val_accuracy: 0.1673\n",
            "Epoch 21/50\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 2.0188 - accuracy: 0.2486 - val_loss: 2.7901 - val_accuracy: 0.1516\n",
            "Epoch 22/50\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 1.9716 - accuracy: 0.2604 - val_loss: 2.8442 - val_accuracy: 0.1602\n",
            "Epoch 23/50\n",
            "140/140 [==============================] - 3s 21ms/step - loss: 1.9657 - accuracy: 0.2659 - val_loss: 2.8380 - val_accuracy: 0.1646\n",
            "Epoch 24/50\n",
            "140/140 [==============================] - 2s 17ms/step - loss: 1.9140 - accuracy: 0.2818 - val_loss: 3.1319 - val_accuracy: 0.1539\n",
            "Epoch 25/50\n",
            "140/140 [==============================] - 3s 23ms/step - loss: 1.8611 - accuracy: 0.2941 - val_loss: 3.0934 - val_accuracy: 0.1355\n",
            "Epoch 26/50\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 1.8398 - accuracy: 0.3048 - val_loss: 3.1662 - val_accuracy: 0.1575\n",
            "Epoch 27/50\n",
            "140/140 [==============================] - 2s 18ms/step - loss: 1.8169 - accuracy: 0.3151 - val_loss: 3.3938 - val_accuracy: 0.1480\n",
            "Epoch 28/50\n",
            "140/140 [==============================] - 2s 17ms/step - loss: 1.7905 - accuracy: 0.3231 - val_loss: 3.3615 - val_accuracy: 0.1530\n",
            "Epoch 29/50\n",
            "140/140 [==============================] - 3s 21ms/step - loss: 1.7684 - accuracy: 0.3388 - val_loss: 3.5390 - val_accuracy: 0.1400\n",
            "Epoch 30/50\n",
            "140/140 [==============================] - 4s 28ms/step - loss: 1.7328 - accuracy: 0.3500 - val_loss: 3.6277 - val_accuracy: 0.1525\n",
            "Epoch 31/50\n",
            "140/140 [==============================] - 3s 22ms/step - loss: 1.7095 - accuracy: 0.3593 - val_loss: 3.7105 - val_accuracy: 0.1615\n",
            "Epoch 32/50\n",
            "140/140 [==============================] - 4s 27ms/step - loss: 1.6911 - accuracy: 0.3705 - val_loss: 3.8294 - val_accuracy: 0.1395\n",
            "Epoch 33/50\n",
            "140/140 [==============================] - 2s 17ms/step - loss: 1.6570 - accuracy: 0.3906 - val_loss: 3.7232 - val_accuracy: 0.1467\n",
            "Epoch 34/50\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 1.6103 - accuracy: 0.4046 - val_loss: 3.9350 - val_accuracy: 0.1521\n",
            "Epoch 35/50\n",
            "140/140 [==============================] - 3s 24ms/step - loss: 1.5835 - accuracy: 0.4108 - val_loss: 4.1186 - val_accuracy: 0.1427\n",
            "Epoch 36/50\n",
            "140/140 [==============================] - 3s 20ms/step - loss: 1.6035 - accuracy: 0.4091 - val_loss: 3.9875 - val_accuracy: 0.1507\n",
            "Epoch 37/50\n",
            "140/140 [==============================] - 2s 17ms/step - loss: 1.5658 - accuracy: 0.4184 - val_loss: 4.2486 - val_accuracy: 0.1391\n",
            "Epoch 38/50\n",
            "140/140 [==============================] - 2s 17ms/step - loss: 1.5504 - accuracy: 0.4362 - val_loss: 4.1249 - val_accuracy: 0.1498\n",
            "Epoch 39/50\n",
            "140/140 [==============================] - 2s 16ms/step - loss: 1.5032 - accuracy: 0.4523 - val_loss: 4.2551 - val_accuracy: 0.1489\n",
            "Epoch 40/50\n",
            "140/140 [==============================] - 3s 21ms/step - loss: 1.4746 - accuracy: 0.4651 - val_loss: 4.4112 - val_accuracy: 0.1494\n",
            "Epoch 41/50\n",
            "140/140 [==============================] - 3s 21ms/step - loss: 1.4710 - accuracy: 0.4641 - val_loss: 4.4230 - val_accuracy: 0.1323\n",
            "Epoch 42/50\n",
            "140/140 [==============================] - 2s 17ms/step - loss: 1.4441 - accuracy: 0.4758 - val_loss: 4.4479 - val_accuracy: 0.1337\n",
            "Epoch 43/50\n",
            "140/140 [==============================] - 2s 17ms/step - loss: 1.4241 - accuracy: 0.4859 - val_loss: 4.5646 - val_accuracy: 0.1449\n",
            "Epoch 44/50\n",
            "140/140 [==============================] - 2s 17ms/step - loss: 1.4146 - accuracy: 0.5012 - val_loss: 4.4114 - val_accuracy: 0.1243\n",
            "Epoch 45/50\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 1.3845 - accuracy: 0.5068 - val_loss: 4.7741 - val_accuracy: 0.1431\n",
            "Epoch 46/50\n",
            "140/140 [==============================] - 3s 24ms/step - loss: 1.3697 - accuracy: 0.5154 - val_loss: 4.5561 - val_accuracy: 0.1368\n",
            "Epoch 47/50\n",
            "140/140 [==============================] - 2s 17ms/step - loss: 1.3903 - accuracy: 0.5136 - val_loss: 4.4749 - val_accuracy: 0.1427\n",
            "Epoch 48/50\n",
            "140/140 [==============================] - 2s 16ms/step - loss: 1.3727 - accuracy: 0.5206 - val_loss: 4.7501 - val_accuracy: 0.1319\n",
            "Epoch 49/50\n",
            "140/140 [==============================] - 2s 17ms/step - loss: 1.3011 - accuracy: 0.5510 - val_loss: 5.1786 - val_accuracy: 0.1382\n",
            "Epoch 50/50\n",
            "140/140 [==============================] - 2s 16ms/step - loss: 1.3194 - accuracy: 0.5322 - val_loss: 5.0194 - val_accuracy: 0.1368\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7af7b7995f90>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dropout, Dense, GlobalAveragePooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Attention\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# Parameters\n",
        "embedding_dim = 64\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Ensure tokenizer is defined previously\n",
        "num_classes = 16  # Set this to the number of your output classes\n",
        "l2_reg = 0.01  # Regularization factor\n",
        "\n",
        "# Model architecture\n",
        "input_text = Input(shape=(100,))\n",
        "embedding_layer = Embedding(vocab_size, embedding_dim, input_length=100)(input_text)\n",
        "bi_lstm = Bidirectional(LSTM(64, return_sequences=True, kernel_regularizer=l2(l2_reg)))(embedding_layer)\n",
        "dropout1 = Dropout(0.5)(bi_lstm)\n",
        "attention = Attention()([dropout1, dropout1])\n",
        "pooling_layer = GlobalAveragePooling1D()(attention)\n",
        "dropout2 = Dropout(0.5)(pooling_layer)\n",
        "dense_layer = Dense(64, activation='relu', kernel_regularizer=l2(l2_reg))(dropout2)\n",
        "output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n",
        "\n",
        "# Create and compile the model\n",
        "model = Model(inputs=input_text, outputs=output_layer)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MoMyhreeFuhj"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(X_train_pad, y_train_cat, epochs=50, batch_size=64, validation_data=(X_test_pad, y_test_cat))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrHqiDhFG-c8",
        "outputId": "b7db8c77-f05d-43d7-ea8d-12a6cc31cd08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "140/140 [==============================] - 20s 100ms/step - loss: 3.5020 - accuracy: 0.1650 - val_loss: 2.6578 - val_accuracy: 0.1853\n",
            "Epoch 2/50\n",
            "140/140 [==============================] - 6s 46ms/step - loss: 2.5448 - accuracy: 0.1701 - val_loss: 2.5031 - val_accuracy: 0.1853\n",
            "Epoch 3/50\n",
            "140/140 [==============================] - 6s 41ms/step - loss: 2.4673 - accuracy: 0.1707 - val_loss: 2.4303 - val_accuracy: 0.1880\n",
            "Epoch 4/50\n",
            "140/140 [==============================] - 4s 30ms/step - loss: 2.4181 - accuracy: 0.1751 - val_loss: 2.4134 - val_accuracy: 0.1920\n",
            "Epoch 5/50\n",
            "140/140 [==============================] - 4s 25ms/step - loss: 2.3604 - accuracy: 0.1881 - val_loss: 2.4110 - val_accuracy: 0.1920\n",
            "Epoch 6/50\n",
            "140/140 [==============================] - 3s 21ms/step - loss: 2.3177 - accuracy: 0.1939 - val_loss: 2.4413 - val_accuracy: 0.1759\n",
            "Epoch 7/50\n",
            "140/140 [==============================] - 4s 26ms/step - loss: 2.3678 - accuracy: 0.1916 - val_loss: 2.4244 - val_accuracy: 0.1857\n",
            "Epoch 8/50\n",
            "140/140 [==============================] - 3s 23ms/step - loss: 2.3692 - accuracy: 0.1870 - val_loss: 2.4229 - val_accuracy: 0.1736\n",
            "Epoch 9/50\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 2.2582 - accuracy: 0.2050 - val_loss: 2.4698 - val_accuracy: 0.1741\n",
            "Epoch 10/50\n",
            "140/140 [==============================] - 3s 20ms/step - loss: 2.2256 - accuracy: 0.2103 - val_loss: 2.5542 - val_accuracy: 0.1606\n",
            "Epoch 11/50\n",
            "140/140 [==============================] - 4s 25ms/step - loss: 2.2001 - accuracy: 0.2124 - val_loss: 2.4314 - val_accuracy: 0.1768\n",
            "Epoch 12/50\n",
            "140/140 [==============================] - 3s 24ms/step - loss: 2.1656 - accuracy: 0.2185 - val_loss: 2.5024 - val_accuracy: 0.1768\n",
            "Epoch 13/50\n",
            "140/140 [==============================] - 3s 20ms/step - loss: 2.1146 - accuracy: 0.2298 - val_loss: 2.6387 - val_accuracy: 0.1669\n",
            "Epoch 14/50\n",
            "140/140 [==============================] - 3s 20ms/step - loss: 2.0879 - accuracy: 0.2339 - val_loss: 2.5718 - val_accuracy: 0.1696\n",
            "Epoch 15/50\n",
            "140/140 [==============================] - 3s 18ms/step - loss: 2.0620 - accuracy: 0.2396 - val_loss: 2.8019 - val_accuracy: 0.1615\n",
            "Epoch 16/50\n",
            "140/140 [==============================] - 4s 25ms/step - loss: 2.0468 - accuracy: 0.2468 - val_loss: 2.7498 - val_accuracy: 0.1566\n",
            "Epoch 17/50\n",
            "140/140 [==============================] - 3s 22ms/step - loss: 2.0327 - accuracy: 0.2438 - val_loss: 2.8584 - val_accuracy: 0.1472\n",
            "Epoch 18/50\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 2.0281 - accuracy: 0.2460 - val_loss: 2.7350 - val_accuracy: 0.1575\n",
            "Epoch 19/50\n",
            "140/140 [==============================] - 2s 18ms/step - loss: 2.0032 - accuracy: 0.2514 - val_loss: 2.6587 - val_accuracy: 0.1525\n",
            "Epoch 20/50\n",
            "140/140 [==============================] - 2s 17ms/step - loss: 1.9868 - accuracy: 0.2556 - val_loss: 2.7712 - val_accuracy: 0.1440\n",
            "Epoch 21/50\n",
            "140/140 [==============================] - 3s 23ms/step - loss: 1.9630 - accuracy: 0.2599 - val_loss: 2.7672 - val_accuracy: 0.1575\n",
            "Epoch 22/50\n",
            "140/140 [==============================] - 3s 22ms/step - loss: 1.9694 - accuracy: 0.2577 - val_loss: 2.8879 - val_accuracy: 0.1584\n",
            "Epoch 23/50\n",
            "140/140 [==============================] - 3s 20ms/step - loss: 1.9685 - accuracy: 0.2523 - val_loss: 2.8627 - val_accuracy: 0.1539\n",
            "Epoch 24/50\n",
            "140/140 [==============================] - 2s 17ms/step - loss: 1.9467 - accuracy: 0.2612 - val_loss: 2.8723 - val_accuracy: 0.1440\n",
            "Epoch 25/50\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 1.9229 - accuracy: 0.2702 - val_loss: 2.8384 - val_accuracy: 0.1498\n",
            "Epoch 26/50\n",
            "140/140 [==============================] - 3s 24ms/step - loss: 1.9067 - accuracy: 0.2689 - val_loss: 2.9521 - val_accuracy: 0.1606\n",
            "Epoch 27/50\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 1.8913 - accuracy: 0.2756 - val_loss: 2.9791 - val_accuracy: 0.1530\n",
            "Epoch 28/50\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 1.8675 - accuracy: 0.2781 - val_loss: 2.9842 - val_accuracy: 0.1498\n",
            "Epoch 29/50\n",
            "140/140 [==============================] - 3s 18ms/step - loss: 1.8867 - accuracy: 0.2726 - val_loss: 2.9820 - val_accuracy: 0.1593\n",
            "Epoch 30/50\n",
            "140/140 [==============================] - 2s 18ms/step - loss: 1.8759 - accuracy: 0.2698 - val_loss: 2.9861 - val_accuracy: 0.1597\n",
            "Epoch 31/50\n",
            "140/140 [==============================] - 3s 21ms/step - loss: 1.8858 - accuracy: 0.2740 - val_loss: 3.0334 - val_accuracy: 0.1427\n",
            "Epoch 32/50\n",
            "140/140 [==============================] - 3s 24ms/step - loss: 1.8603 - accuracy: 0.2755 - val_loss: 3.0902 - val_accuracy: 0.1489\n",
            "Epoch 33/50\n",
            "140/140 [==============================] - 3s 18ms/step - loss: 1.8320 - accuracy: 0.2817 - val_loss: 3.1067 - val_accuracy: 0.1570\n",
            "Epoch 34/50\n",
            "140/140 [==============================] - 2s 17ms/step - loss: 1.8224 - accuracy: 0.2829 - val_loss: 3.1476 - val_accuracy: 0.1597\n",
            "Epoch 35/50\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 1.8211 - accuracy: 0.2819 - val_loss: 3.0637 - val_accuracy: 0.1579\n",
            "Epoch 36/50\n",
            "140/140 [==============================] - 3s 20ms/step - loss: 1.7964 - accuracy: 0.2876 - val_loss: 3.1901 - val_accuracy: 0.1570\n",
            "Epoch 37/50\n",
            "140/140 [==============================] - 3s 23ms/step - loss: 1.8424 - accuracy: 0.2789 - val_loss: 3.1418 - val_accuracy: 0.1597\n",
            "Epoch 38/50\n",
            "140/140 [==============================] - 2s 17ms/step - loss: 1.7803 - accuracy: 0.2939 - val_loss: 3.1706 - val_accuracy: 0.1642\n",
            "Epoch 39/50\n",
            "140/140 [==============================] - 3s 18ms/step - loss: 1.7859 - accuracy: 0.2920 - val_loss: 3.1779 - val_accuracy: 0.1480\n",
            "Epoch 40/50\n",
            "140/140 [==============================] - 2s 17ms/step - loss: 1.7727 - accuracy: 0.2937 - val_loss: 3.2683 - val_accuracy: 0.1624\n",
            "Epoch 41/50\n",
            "140/140 [==============================] - 3s 20ms/step - loss: 1.7400 - accuracy: 0.3049 - val_loss: 3.2722 - val_accuracy: 0.1467\n",
            "Epoch 42/50\n",
            "140/140 [==============================] - 3s 24ms/step - loss: 1.7474 - accuracy: 0.3031 - val_loss: 3.4294 - val_accuracy: 0.1516\n",
            "Epoch 43/50\n",
            "140/140 [==============================] - 2s 17ms/step - loss: 1.7270 - accuracy: 0.3057 - val_loss: 3.3303 - val_accuracy: 0.1566\n",
            "Epoch 44/50\n",
            "140/140 [==============================] - 2s 18ms/step - loss: 1.7631 - accuracy: 0.3041 - val_loss: 3.3529 - val_accuracy: 0.1588\n",
            "Epoch 45/50\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 1.7097 - accuracy: 0.3160 - val_loss: 3.5515 - val_accuracy: 0.1440\n",
            "Epoch 46/50\n",
            "140/140 [==============================] - 4s 25ms/step - loss: 1.7095 - accuracy: 0.3115 - val_loss: 3.4661 - val_accuracy: 0.1534\n",
            "Epoch 47/50\n",
            "140/140 [==============================] - 3s 24ms/step - loss: 1.7081 - accuracy: 0.3141 - val_loss: 3.5111 - val_accuracy: 0.1449\n",
            "Epoch 48/50\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 1.6737 - accuracy: 0.3207 - val_loss: 3.4579 - val_accuracy: 0.1588\n",
            "Epoch 49/50\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 1.6768 - accuracy: 0.3236 - val_loss: 3.3528 - val_accuracy: 0.1494\n",
            "Epoch 50/50\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 1.6827 - accuracy: 0.3240 - val_loss: 3.6217 - val_accuracy: 0.1552\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7af738ddda50>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Another Trial**"
      ],
      "metadata": {
        "id": "MC78aaINLuEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nlpaug"
      ],
      "metadata": {
        "id": "ZZe26Rv_Jq3K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "919f0254-2507-4f58-ee41-cc6c068b9b32"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nlpaug\n",
            "  Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/410.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/410.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.31.0)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (5.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (3.13.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.66.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n",
            "Installing collected packages: nlpaug\n",
            "Successfully installed nlpaug-1.1.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dropout, Dense, GlobalAveragePooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Attention\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from nlpaug.augmenter.word import SynonymAug\n"
      ],
      "metadata": {
        "id": "70xra6tpJhp9"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label Encoding\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)"
      ],
      "metadata": {
        "id": "s92iKYTcLf2e"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "embedding_dim = 64\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "num_classes = len(np.unique(y_train_encoded))  # This dynamically sets the number of classes\n",
        "l2_reg = 0.01"
      ],
      "metadata": {
        "id": "kRNriBG_LhGI"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Augmentation\n",
        "aug = SynonymAug(aug_src='wordnet')\n",
        "def augment_text(texts, labels, augmenter, num_augmented=2):\n",
        "    augmented_texts = []\n",
        "    augmented_labels = []\n",
        "    for text, label in zip(texts, labels):\n",
        "        augmented_texts.append(text)\n",
        "        augmented_labels.append(label)\n",
        "        for _ in range(num_augmented):\n",
        "            augmented_text = augmenter.augment(text)\n",
        "            augmented_texts.append(augmented_text)\n",
        "            augmented_labels.append(label)\n",
        "    return augmented_texts, augmented_labels"
      ],
      "metadata": {
        "id": "vdAcOfmDLicb"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_aug, y_train_aug = augment_text(X_train, y_train_encoded, aug, num_augmented=1)\n",
        "X_train_aug_seq = tokenizer.texts_to_sequences(X_train_aug)\n",
        "X_train_aug_pad = pad_sequences(X_train_aug_seq, maxlen=100)\n",
        "y_train_aug_cat = to_categorical(y_train_aug, num_classes=num_classes)"
      ],
      "metadata": {
        "id": "gE5mL2LiLjw1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e1ae6b5-512f-45b4-ea34-2dedf06017e0"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Definition\n",
        "input_text = Input(shape=(100,))\n",
        "embedding_layer = Embedding(vocab_size, embedding_dim, input_length=100)(input_text)\n",
        "bi_lstm = Bidirectional(LSTM(64, return_sequences=True, kernel_regularizer=l2(l2_reg)))(embedding_layer)\n",
        "dropout1 = Dropout(0.5)(bi_lstm)\n",
        "attention = Attention()([dropout1, dropout1])\n",
        "pooling_layer = GlobalAveragePooling1D()(attention)\n",
        "dropout2 = Dropout(0.5)(pooling_layer)\n",
        "dense_layer = Dense(64, activation='relu', kernel_regularizer=l2(l2_reg))(dropout2)\n",
        "output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n"
      ],
      "metadata": {
        "id": "jb2LEsrFLlUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(inputs=input_text, outputs=output_layer)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train_aug_pad, y_train_aug_cat, epochs=50, batch_size=64, validation_data=(X_test_pad, y_test_cat))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nc9OCdKqLmZc",
        "outputId": "7acd5a89-9eac-4b87-8240-c426ea161cbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "279/279 [==============================] - 47s 153ms/step - loss: 3.0514 - accuracy: 0.1707 - val_loss: 2.4931 - val_accuracy: 0.1853\n",
            "Epoch 2/50\n",
            "279/279 [==============================] - 26s 94ms/step - loss: 2.5062 - accuracy: 0.1719 - val_loss: 2.4447 - val_accuracy: 0.1875\n",
            "Epoch 3/50\n",
            "279/279 [==============================] - 18s 63ms/step - loss: 2.4857 - accuracy: 0.1733 - val_loss: 2.4093 - val_accuracy: 0.1853\n",
            "Epoch 4/50\n",
            "279/279 [==============================] - 12s 45ms/step - loss: 2.4603 - accuracy: 0.1732 - val_loss: 2.4250 - val_accuracy: 0.1853\n",
            "Epoch 5/50\n",
            "279/279 [==============================] - 12s 42ms/step - loss: 2.4578 - accuracy: 0.1777 - val_loss: 2.4465 - val_accuracy: 0.1911\n",
            "Epoch 6/50\n",
            "279/279 [==============================] - 9s 33ms/step - loss: 2.4748 - accuracy: 0.1808 - val_loss: 2.4754 - val_accuracy: 0.1808\n",
            "Epoch 7/50\n",
            "279/279 [==============================] - 7s 25ms/step - loss: 2.4833 - accuracy: 0.1790 - val_loss: 2.4675 - val_accuracy: 0.1786\n",
            "Epoch 8/50\n",
            "279/279 [==============================] - 9s 31ms/step - loss: 2.4939 - accuracy: 0.1744 - val_loss: 2.5061 - val_accuracy: 0.1853\n",
            "Epoch 9/50\n",
            "279/279 [==============================] - 8s 27ms/step - loss: 2.5022 - accuracy: 0.1753 - val_loss: 2.4908 - val_accuracy: 0.1853\n",
            "Epoch 10/50\n",
            "279/279 [==============================] - 8s 30ms/step - loss: 2.4843 - accuracy: 0.1752 - val_loss: 2.4544 - val_accuracy: 0.1875\n",
            "Epoch 11/50\n",
            "279/279 [==============================] - 6s 21ms/step - loss: 2.4935 - accuracy: 0.1771 - val_loss: 2.4731 - val_accuracy: 0.1848\n",
            "Epoch 12/50\n",
            "279/279 [==============================] - 6s 23ms/step - loss: 2.4941 - accuracy: 0.1779 - val_loss: 2.4825 - val_accuracy: 0.1866\n",
            "Epoch 13/50\n",
            "279/279 [==============================] - 6s 22ms/step - loss: 2.4956 - accuracy: 0.1778 - val_loss: 2.4776 - val_accuracy: 0.1763\n",
            "Epoch 14/50\n",
            "279/279 [==============================] - 7s 25ms/step - loss: 2.4820 - accuracy: 0.1766 - val_loss: 2.4674 - val_accuracy: 0.1548\n",
            "Epoch 15/50\n",
            "279/279 [==============================] - 6s 22ms/step - loss: 2.4933 - accuracy: 0.1763 - val_loss: 2.4866 - val_accuracy: 0.1853\n",
            "Epoch 16/50\n",
            "279/279 [==============================] - 7s 24ms/step - loss: 2.4981 - accuracy: 0.1755 - val_loss: 2.4735 - val_accuracy: 0.1853\n",
            "Epoch 17/50\n",
            "279/279 [==============================] - 6s 22ms/step - loss: 2.4813 - accuracy: 0.1751 - val_loss: 2.4504 - val_accuracy: 0.1853\n",
            "Epoch 18/50\n",
            "279/279 [==============================] - 6s 21ms/step - loss: 2.4656 - accuracy: 0.1755 - val_loss: 2.4349 - val_accuracy: 0.1853\n",
            "Epoch 19/50\n",
            "279/279 [==============================] - 7s 24ms/step - loss: 2.4396 - accuracy: 0.1755 - val_loss: 2.4106 - val_accuracy: 0.1853\n",
            "Epoch 20/50\n",
            "279/279 [==============================] - 5s 19ms/step - loss: 2.4336 - accuracy: 0.1764 - val_loss: 2.4190 - val_accuracy: 0.1812\n",
            "Epoch 21/50\n",
            "279/279 [==============================] - 7s 24ms/step - loss: 2.4515 - accuracy: 0.1707 - val_loss: 2.4129 - val_accuracy: 0.1853\n",
            "Epoch 22/50\n",
            "279/279 [==============================] - 6s 20ms/step - loss: 2.4309 - accuracy: 0.1755 - val_loss: 2.4381 - val_accuracy: 0.1853\n",
            "Epoch 23/50\n",
            "279/279 [==============================] - 6s 22ms/step - loss: 2.3936 - accuracy: 0.1751 - val_loss: 2.4670 - val_accuracy: 0.1853\n",
            "Epoch 24/50\n",
            "279/279 [==============================] - 6s 22ms/step - loss: 2.3718 - accuracy: 0.1835 - val_loss: 2.4465 - val_accuracy: 0.1853\n",
            "Epoch 25/50\n",
            "279/279 [==============================] - 6s 22ms/step - loss: 2.4309 - accuracy: 0.1755 - val_loss: 2.5106 - val_accuracy: 0.1853\n",
            "Epoch 26/50\n",
            "279/279 [==============================] - 6s 22ms/step - loss: 2.3802 - accuracy: 0.1822 - val_loss: 2.5257 - val_accuracy: 0.1772\n",
            "Epoch 27/50\n",
            "279/279 [==============================] - 5s 19ms/step - loss: 2.3742 - accuracy: 0.1824 - val_loss: 2.4957 - val_accuracy: 0.1839\n",
            "Epoch 28/50\n",
            "279/279 [==============================] - 6s 22ms/step - loss: 2.3705 - accuracy: 0.1839 - val_loss: 2.5112 - val_accuracy: 0.1853\n",
            "Epoch 29/50\n",
            "279/279 [==============================] - 6s 21ms/step - loss: 2.3663 - accuracy: 0.1859 - val_loss: 2.4557 - val_accuracy: 0.1853\n",
            "Epoch 30/50\n",
            "279/279 [==============================] - 6s 22ms/step - loss: 2.3963 - accuracy: 0.1808 - val_loss: 2.4985 - val_accuracy: 0.1875\n",
            "Epoch 31/50\n",
            "279/279 [==============================] - 8s 28ms/step - loss: 2.3281 - accuracy: 0.1906 - val_loss: 2.5460 - val_accuracy: 0.1768\n",
            "Epoch 32/50\n",
            "279/279 [==============================] - 5s 17ms/step - loss: 2.3169 - accuracy: 0.1934 - val_loss: 2.5241 - val_accuracy: 0.1857\n",
            "Epoch 33/50\n",
            "279/279 [==============================] - 6s 23ms/step - loss: 2.3524 - accuracy: 0.1885 - val_loss: 2.8955 - val_accuracy: 0.1629\n",
            "Epoch 34/50\n",
            "279/279 [==============================] - 5s 19ms/step - loss: 2.3594 - accuracy: 0.1904 - val_loss: 2.6264 - val_accuracy: 0.1916\n",
            "Epoch 35/50\n",
            "279/279 [==============================] - 5s 18ms/step - loss: 2.3102 - accuracy: 0.1968 - val_loss: 2.6148 - val_accuracy: 0.1709\n",
            "Epoch 36/50\n",
            "279/279 [==============================] - 6s 23ms/step - loss: 2.3048 - accuracy: 0.1947 - val_loss: 2.8415 - val_accuracy: 0.1705\n",
            "Epoch 37/50\n",
            "279/279 [==============================] - 5s 17ms/step - loss: 2.2978 - accuracy: 0.1962 - val_loss: 2.6981 - val_accuracy: 0.1763\n",
            "Epoch 38/50\n",
            "279/279 [==============================] - 5s 19ms/step - loss: 2.3321 - accuracy: 0.1926 - val_loss: 2.4264 - val_accuracy: 0.1893\n",
            "Epoch 39/50\n",
            "279/279 [==============================] - 5s 19ms/step - loss: 2.3248 - accuracy: 0.1932 - val_loss: 2.6151 - val_accuracy: 0.1700\n",
            "Epoch 40/50\n",
            "279/279 [==============================] - 5s 18ms/step - loss: 2.2896 - accuracy: 0.1993 - val_loss: 2.6817 - val_accuracy: 0.1790\n",
            "Epoch 41/50\n",
            "279/279 [==============================] - 6s 23ms/step - loss: 2.3872 - accuracy: 0.1857 - val_loss: 2.4386 - val_accuracy: 0.1839\n",
            "Epoch 42/50\n",
            "279/279 [==============================] - 5s 18ms/step - loss: 2.3617 - accuracy: 0.1918 - val_loss: 2.6117 - val_accuracy: 0.1741\n",
            "Epoch 43/50\n",
            "279/279 [==============================] - 5s 18ms/step - loss: 2.2956 - accuracy: 0.1996 - val_loss: 2.7497 - val_accuracy: 0.1700\n",
            "Epoch 44/50\n",
            "279/279 [==============================] - 6s 22ms/step - loss: 2.2815 - accuracy: 0.2034 - val_loss: 2.8623 - val_accuracy: 0.1602\n",
            "Epoch 45/50\n",
            "279/279 [==============================] - 5s 18ms/step - loss: 2.2910 - accuracy: 0.2047 - val_loss: 2.8052 - val_accuracy: 0.1633\n",
            "Epoch 46/50\n",
            "279/279 [==============================] - 5s 20ms/step - loss: 2.3003 - accuracy: 0.2008 - val_loss: 2.8047 - val_accuracy: 0.1552\n",
            "Epoch 47/50\n",
            "279/279 [==============================] - 5s 19ms/step - loss: 2.2651 - accuracy: 0.2044 - val_loss: 2.9445 - val_accuracy: 0.1606\n",
            "Epoch 48/50\n",
            "279/279 [==============================] - 5s 17ms/step - loss: 2.2771 - accuracy: 0.2059 - val_loss: 2.6252 - val_accuracy: 0.1620\n",
            "Epoch 49/50\n",
            "279/279 [==============================] - 6s 21ms/step - loss: 2.2764 - accuracy: 0.2052 - val_loss: 2.9519 - val_accuracy: 0.1593\n",
            "Epoch 50/50\n",
            "279/279 [==============================] - 5s 18ms/step - loss: 2.2641 - accuracy: 0.2056 - val_loss: 2.9022 - val_accuracy: 0.1732\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7af68597e170>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_aug_pad, y_train_aug_cat, epochs=100, batch_size=128, validation_data=(X_test_pad, y_test_cat))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5FILA8KLo2v",
        "outputId": "4df2ecd1-a85b-4d04-e485-deca090f9a65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "140/140 [==============================] - 20s 139ms/step - loss: 2.2535 - accuracy: 0.2031 - val_loss: 2.9325 - val_accuracy: 0.1655\n",
            "Epoch 2/100\n",
            "140/140 [==============================] - 16s 111ms/step - loss: 2.2413 - accuracy: 0.2121 - val_loss: 3.0665 - val_accuracy: 0.1624\n",
            "Epoch 3/100\n",
            "140/140 [==============================] - 10s 75ms/step - loss: 2.2205 - accuracy: 0.2167 - val_loss: 3.1677 - val_accuracy: 0.1606\n",
            "Epoch 4/100\n",
            "140/140 [==============================] - 10s 69ms/step - loss: 2.2203 - accuracy: 0.2211 - val_loss: 3.1819 - val_accuracy: 0.1525\n",
            "Epoch 5/100\n",
            "140/140 [==============================] - 8s 58ms/step - loss: 2.2152 - accuracy: 0.2196 - val_loss: 3.0051 - val_accuracy: 0.1575\n",
            "Epoch 6/100\n",
            "140/140 [==============================] - 7s 49ms/step - loss: 2.2163 - accuracy: 0.2206 - val_loss: 3.1314 - val_accuracy: 0.1651\n",
            "Epoch 7/100\n",
            "140/140 [==============================] - 7s 51ms/step - loss: 2.2090 - accuracy: 0.2212 - val_loss: 3.0692 - val_accuracy: 0.1593\n",
            "Epoch 8/100\n",
            "140/140 [==============================] - 7s 52ms/step - loss: 2.2545 - accuracy: 0.2120 - val_loss: 3.1830 - val_accuracy: 0.1642\n",
            "Epoch 9/100\n",
            "140/140 [==============================] - 5s 31ms/step - loss: 2.2212 - accuracy: 0.2194 - val_loss: 2.9578 - val_accuracy: 0.1584\n",
            "Epoch 10/100\n",
            "140/140 [==============================] - 4s 28ms/step - loss: 2.2082 - accuracy: 0.2204 - val_loss: 3.0682 - val_accuracy: 0.1476\n",
            "Epoch 11/100\n",
            "140/140 [==============================] - 7s 53ms/step - loss: 2.1965 - accuracy: 0.2245 - val_loss: 3.0446 - val_accuracy: 0.1507\n",
            "Epoch 12/100\n",
            "140/140 [==============================] - 4s 26ms/step - loss: 2.2212 - accuracy: 0.2163 - val_loss: 3.1944 - val_accuracy: 0.1516\n",
            "Epoch 13/100\n",
            "140/140 [==============================] - 3s 25ms/step - loss: 2.2130 - accuracy: 0.2212 - val_loss: 2.9932 - val_accuracy: 0.1696\n",
            "Epoch 14/100\n",
            "140/140 [==============================] - 5s 32ms/step - loss: 2.2061 - accuracy: 0.2191 - val_loss: 3.1252 - val_accuracy: 0.1480\n",
            "Epoch 15/100\n",
            "140/140 [==============================] - 5s 37ms/step - loss: 2.1883 - accuracy: 0.2237 - val_loss: 3.1378 - val_accuracy: 0.1350\n",
            "Epoch 16/100\n",
            "140/140 [==============================] - 4s 26ms/step - loss: 2.1826 - accuracy: 0.2253 - val_loss: 3.2042 - val_accuracy: 0.1602\n",
            "Epoch 17/100\n",
            "140/140 [==============================] - 3s 24ms/step - loss: 2.1856 - accuracy: 0.2243 - val_loss: 3.2115 - val_accuracy: 0.1467\n",
            "Epoch 18/100\n",
            "140/140 [==============================] - 5s 33ms/step - loss: 2.1929 - accuracy: 0.2239 - val_loss: 3.0886 - val_accuracy: 0.1485\n",
            "Epoch 19/100\n",
            "140/140 [==============================] - 4s 29ms/step - loss: 2.1821 - accuracy: 0.2271 - val_loss: 3.2406 - val_accuracy: 0.1539\n",
            "Epoch 20/100\n",
            "140/140 [==============================] - 3s 20ms/step - loss: 2.1758 - accuracy: 0.2257 - val_loss: 3.2507 - val_accuracy: 0.1422\n",
            "Epoch 21/100\n",
            "140/140 [==============================] - 3s 21ms/step - loss: 2.1704 - accuracy: 0.2253 - val_loss: 3.2110 - val_accuracy: 0.1552\n",
            "Epoch 22/100\n",
            "140/140 [==============================] - 4s 29ms/step - loss: 2.1759 - accuracy: 0.2255 - val_loss: 3.1112 - val_accuracy: 0.1534\n",
            "Epoch 23/100\n",
            "140/140 [==============================] - 4s 28ms/step - loss: 2.1735 - accuracy: 0.2259 - val_loss: 3.3187 - val_accuracy: 0.1575\n",
            "Epoch 24/100\n",
            "140/140 [==============================] - 3s 20ms/step - loss: 2.1829 - accuracy: 0.2276 - val_loss: 3.2249 - val_accuracy: 0.1355\n",
            "Epoch 25/100\n",
            "140/140 [==============================] - 3s 24ms/step - loss: 2.1825 - accuracy: 0.2260 - val_loss: 3.3039 - val_accuracy: 0.1606\n",
            "Epoch 26/100\n",
            "140/140 [==============================] - 3s 24ms/step - loss: 2.1601 - accuracy: 0.2300 - val_loss: 3.3892 - val_accuracy: 0.1588\n",
            "Epoch 27/100\n",
            "140/140 [==============================] - 4s 29ms/step - loss: 2.2056 - accuracy: 0.2235 - val_loss: 3.1335 - val_accuracy: 0.1337\n",
            "Epoch 28/100\n",
            "140/140 [==============================] - 3s 20ms/step - loss: 2.1673 - accuracy: 0.2296 - val_loss: 3.2761 - val_accuracy: 0.1772\n",
            "Epoch 29/100\n",
            "140/140 [==============================] - 3s 22ms/step - loss: 2.1656 - accuracy: 0.2271 - val_loss: 3.2640 - val_accuracy: 0.1705\n",
            "Epoch 30/100\n",
            "140/140 [==============================] - 3s 20ms/step - loss: 2.1714 - accuracy: 0.2262 - val_loss: 3.2688 - val_accuracy: 0.1534\n",
            "Epoch 31/100\n",
            "140/140 [==============================] - 4s 28ms/step - loss: 2.1507 - accuracy: 0.2299 - val_loss: 3.2831 - val_accuracy: 0.1485\n",
            "Epoch 32/100\n",
            "140/140 [==============================] - 4s 28ms/step - loss: 2.2093 - accuracy: 0.2202 - val_loss: 2.9041 - val_accuracy: 0.1261\n",
            "Epoch 33/100\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 2.2259 - accuracy: 0.2168 - val_loss: 3.1894 - val_accuracy: 0.1395\n",
            "Epoch 34/100\n",
            "140/140 [==============================] - 3s 23ms/step - loss: 2.1760 - accuracy: 0.2300 - val_loss: 3.2843 - val_accuracy: 0.1301\n",
            "Epoch 35/100\n",
            "140/140 [==============================] - 4s 26ms/step - loss: 2.1716 - accuracy: 0.2285 - val_loss: 3.2836 - val_accuracy: 0.1323\n",
            "Epoch 36/100\n",
            "140/140 [==============================] - 3s 22ms/step - loss: 2.1472 - accuracy: 0.2309 - val_loss: 3.2955 - val_accuracy: 0.1615\n",
            "Epoch 37/100\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 2.1620 - accuracy: 0.2299 - val_loss: 3.3074 - val_accuracy: 0.1386\n",
            "Epoch 38/100\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 2.1471 - accuracy: 0.2317 - val_loss: 3.1971 - val_accuracy: 0.1391\n",
            "Epoch 39/100\n",
            "140/140 [==============================] - 3s 24ms/step - loss: 2.1822 - accuracy: 0.2260 - val_loss: 2.9598 - val_accuracy: 0.1489\n",
            "Epoch 40/100\n",
            "140/140 [==============================] - 4s 32ms/step - loss: 2.1948 - accuracy: 0.2272 - val_loss: 3.2443 - val_accuracy: 0.1467\n",
            "Epoch 41/100\n",
            "140/140 [==============================] - 4s 25ms/step - loss: 2.1786 - accuracy: 0.2317 - val_loss: 3.0054 - val_accuracy: 0.1633\n",
            "Epoch 42/100\n",
            "140/140 [==============================] - 3s 21ms/step - loss: 2.1780 - accuracy: 0.2263 - val_loss: 3.3076 - val_accuracy: 0.1561\n",
            "Epoch 43/100\n",
            "140/140 [==============================] - 3s 24ms/step - loss: 2.1693 - accuracy: 0.2306 - val_loss: 2.9735 - val_accuracy: 0.1283\n",
            "Epoch 44/100\n",
            "140/140 [==============================] - 5s 33ms/step - loss: 2.4075 - accuracy: 0.1804 - val_loss: 2.7852 - val_accuracy: 0.1216\n",
            "Epoch 45/100\n",
            "140/140 [==============================] - 3s 20ms/step - loss: 2.2336 - accuracy: 0.2194 - val_loss: 3.3884 - val_accuracy: 0.1507\n",
            "Epoch 46/100\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 2.1615 - accuracy: 0.2346 - val_loss: 3.3579 - val_accuracy: 0.1530\n",
            "Epoch 47/100\n",
            "140/140 [==============================] - 3s 22ms/step - loss: 2.1408 - accuracy: 0.2389 - val_loss: 3.3664 - val_accuracy: 0.1642\n",
            "Epoch 48/100\n",
            "140/140 [==============================] - 4s 25ms/step - loss: 2.1352 - accuracy: 0.2381 - val_loss: 3.2624 - val_accuracy: 0.1297\n",
            "Epoch 49/100\n",
            "140/140 [==============================] - 3s 24ms/step - loss: 2.1391 - accuracy: 0.2367 - val_loss: 3.2251 - val_accuracy: 0.1175\n",
            "Epoch 50/100\n",
            "140/140 [==============================] - 3s 23ms/step - loss: 2.1380 - accuracy: 0.2380 - val_loss: 3.3252 - val_accuracy: 0.1306\n",
            "Epoch 51/100\n",
            "140/140 [==============================] - 3s 21ms/step - loss: 2.1304 - accuracy: 0.2437 - val_loss: 3.2993 - val_accuracy: 0.1328\n",
            "Epoch 52/100\n",
            "140/140 [==============================] - 3s 23ms/step - loss: 2.1431 - accuracy: 0.2407 - val_loss: 3.3884 - val_accuracy: 0.1261\n",
            "Epoch 53/100\n",
            "140/140 [==============================] - 4s 28ms/step - loss: 2.2434 - accuracy: 0.2189 - val_loss: 3.2885 - val_accuracy: 0.1507\n",
            "Epoch 54/100\n",
            "140/140 [==============================] - 3s 23ms/step - loss: 2.1424 - accuracy: 0.2383 - val_loss: 3.2880 - val_accuracy: 0.1543\n",
            "Epoch 55/100\n",
            "140/140 [==============================] - 3s 21ms/step - loss: 2.1319 - accuracy: 0.2415 - val_loss: 3.3703 - val_accuracy: 0.1144\n",
            "Epoch 56/100\n",
            "140/140 [==============================] - 4s 27ms/step - loss: 2.2243 - accuracy: 0.2176 - val_loss: 3.1503 - val_accuracy: 0.1400\n",
            "Epoch 57/100\n",
            "140/140 [==============================] - 4s 27ms/step - loss: 2.1881 - accuracy: 0.2258 - val_loss: 3.0781 - val_accuracy: 0.1673\n",
            "Epoch 58/100\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 2.1674 - accuracy: 0.2379 - val_loss: 3.1950 - val_accuracy: 0.1669\n",
            "Epoch 59/100\n",
            "140/140 [==============================] - 3s 18ms/step - loss: 2.1415 - accuracy: 0.2422 - val_loss: 3.3178 - val_accuracy: 0.1597\n",
            "Epoch 60/100\n",
            "140/140 [==============================] - 3s 23ms/step - loss: 2.1510 - accuracy: 0.2412 - val_loss: 3.3135 - val_accuracy: 0.1476\n",
            "Epoch 61/100\n",
            "140/140 [==============================] - 3s 23ms/step - loss: 2.1572 - accuracy: 0.2392 - val_loss: 3.2075 - val_accuracy: 0.1552\n",
            "Epoch 62/100\n",
            "140/140 [==============================] - 3s 21ms/step - loss: 2.1248 - accuracy: 0.2491 - val_loss: 3.3287 - val_accuracy: 0.1575\n",
            "Epoch 63/100\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 2.1561 - accuracy: 0.2418 - val_loss: 3.2912 - val_accuracy: 0.1615\n",
            "Epoch 64/100\n",
            "140/140 [==============================] - 3s 24ms/step - loss: 2.1521 - accuracy: 0.2453 - val_loss: 3.0491 - val_accuracy: 0.1723\n",
            "Epoch 65/100\n",
            "140/140 [==============================] - 3s 24ms/step - loss: 2.2486 - accuracy: 0.2214 - val_loss: 3.1508 - val_accuracy: 0.1642\n",
            "Epoch 66/100\n",
            "140/140 [==============================] - 4s 28ms/step - loss: 2.1424 - accuracy: 0.2512 - val_loss: 3.3533 - val_accuracy: 0.1449\n",
            "Epoch 67/100\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 2.1179 - accuracy: 0.2522 - val_loss: 3.3984 - val_accuracy: 0.1660\n",
            "Epoch 68/100\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 2.1843 - accuracy: 0.2345 - val_loss: 3.2924 - val_accuracy: 0.1548\n",
            "Epoch 69/100\n",
            "140/140 [==============================] - 3s 24ms/step - loss: 2.1394 - accuracy: 0.2494 - val_loss: 3.2932 - val_accuracy: 0.1611\n",
            "Epoch 70/100\n",
            "140/140 [==============================] - 4s 27ms/step - loss: 2.1129 - accuracy: 0.2563 - val_loss: 3.4110 - val_accuracy: 0.1516\n",
            "Epoch 71/100\n",
            "140/140 [==============================] - 3s 21ms/step - loss: 2.0945 - accuracy: 0.2592 - val_loss: 3.4996 - val_accuracy: 0.1602\n",
            "Epoch 72/100\n",
            "140/140 [==============================] - 3s 21ms/step - loss: 2.2825 - accuracy: 0.2166 - val_loss: 3.2703 - val_accuracy: 0.1588\n",
            "Epoch 73/100\n",
            "140/140 [==============================] - 3s 21ms/step - loss: 2.1430 - accuracy: 0.2508 - val_loss: 3.4665 - val_accuracy: 0.1566\n",
            "Epoch 74/100\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 2.1182 - accuracy: 0.2588 - val_loss: 3.0699 - val_accuracy: 0.1638\n",
            "Epoch 75/100\n",
            "140/140 [==============================] - 3s 23ms/step - loss: 2.1514 - accuracy: 0.2563 - val_loss: 3.3941 - val_accuracy: 0.1696\n",
            "Epoch 76/100\n",
            "140/140 [==============================] - 3s 21ms/step - loss: 2.1327 - accuracy: 0.2604 - val_loss: 3.4140 - val_accuracy: 0.1597\n",
            "Epoch 77/100\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 2.1276 - accuracy: 0.2607 - val_loss: 3.5419 - val_accuracy: 0.1575\n",
            "Epoch 78/100\n",
            "140/140 [==============================] - 3s 18ms/step - loss: 2.1020 - accuracy: 0.2658 - val_loss: 3.6233 - val_accuracy: 0.1579\n",
            "Epoch 79/100\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 2.1287 - accuracy: 0.2607 - val_loss: 3.4622 - val_accuracy: 0.1552\n",
            "Epoch 80/100\n",
            "140/140 [==============================] - 3s 25ms/step - loss: 2.2267 - accuracy: 0.2334 - val_loss: 3.1615 - val_accuracy: 0.1750\n",
            "Epoch 81/100\n",
            "140/140 [==============================] - 3s 21ms/step - loss: 2.2254 - accuracy: 0.2473 - val_loss: 3.1910 - val_accuracy: 0.1615\n",
            "Epoch 82/100\n",
            "140/140 [==============================] - 3s 20ms/step - loss: 2.1550 - accuracy: 0.2602 - val_loss: 3.3353 - val_accuracy: 0.1552\n",
            "Epoch 83/100\n",
            "140/140 [==============================] - 3s 20ms/step - loss: 2.2867 - accuracy: 0.2361 - val_loss: 3.0233 - val_accuracy: 0.1377\n",
            "Epoch 84/100\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 2.3364 - accuracy: 0.2204 - val_loss: 3.0897 - val_accuracy: 0.1543\n",
            "Epoch 85/100\n",
            "140/140 [==============================] - 3s 25ms/step - loss: 2.2723 - accuracy: 0.2451 - val_loss: 3.0109 - val_accuracy: 0.1633\n",
            "Epoch 86/100\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 2.1733 - accuracy: 0.2588 - val_loss: 3.2406 - val_accuracy: 0.1696\n",
            "Epoch 87/100\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 2.1282 - accuracy: 0.2694 - val_loss: 3.5401 - val_accuracy: 0.1503\n",
            "Epoch 88/100\n",
            "140/140 [==============================] - 2s 17ms/step - loss: 2.1230 - accuracy: 0.2701 - val_loss: 3.5960 - val_accuracy: 0.1494\n",
            "Epoch 89/100\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 2.1174 - accuracy: 0.2707 - val_loss: 3.6453 - val_accuracy: 0.1489\n",
            "Epoch 90/100\n",
            "140/140 [==============================] - 4s 28ms/step - loss: 2.0816 - accuracy: 0.2797 - val_loss: 3.5689 - val_accuracy: 0.1597\n",
            "Epoch 91/100\n",
            "140/140 [==============================] - 4s 30ms/step - loss: 2.0842 - accuracy: 0.2791 - val_loss: 3.7583 - val_accuracy: 0.1494\n",
            "Epoch 92/100\n",
            "140/140 [==============================] - 3s 18ms/step - loss: 2.0648 - accuracy: 0.2827 - val_loss: 3.7350 - val_accuracy: 0.1507\n",
            "Epoch 93/100\n",
            "140/140 [==============================] - 3s 19ms/step - loss: 2.0555 - accuracy: 0.2864 - val_loss: 3.8116 - val_accuracy: 0.1480\n",
            "Epoch 94/100\n",
            "140/140 [==============================] - 3s 21ms/step - loss: 2.0588 - accuracy: 0.2856 - val_loss: 3.6962 - val_accuracy: 0.1521\n",
            "Epoch 95/100\n",
            "140/140 [==============================] - 3s 23ms/step - loss: 2.0401 - accuracy: 0.2921 - val_loss: 3.9317 - val_accuracy: 0.1489\n",
            "Epoch 96/100\n",
            "140/140 [==============================] - 3s 21ms/step - loss: 2.0655 - accuracy: 0.2875 - val_loss: 3.8636 - val_accuracy: 0.1503\n",
            "Epoch 97/100\n",
            "140/140 [==============================] - 3s 21ms/step - loss: 2.0509 - accuracy: 0.2907 - val_loss: 3.1487 - val_accuracy: 0.1741\n",
            "Epoch 98/100\n",
            "140/140 [==============================] - 3s 20ms/step - loss: 2.2077 - accuracy: 0.2631 - val_loss: 3.3083 - val_accuracy: 0.1498\n",
            "Epoch 99/100\n",
            "140/140 [==============================] - 3s 22ms/step - loss: 2.0807 - accuracy: 0.2882 - val_loss: 3.7113 - val_accuracy: 0.1521\n",
            "Epoch 100/100\n",
            "140/140 [==============================] - 3s 23ms/step - loss: 2.0361 - accuracy: 0.2979 - val_loss: 3.7583 - val_accuracy: 0.1548\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7af5ac1ff820>"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_aug_pad, y_train_aug_cat, epochs=200, batch_size=512, validation_data=(X_test_pad, y_test_cat))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgbsH-8UPGdV",
        "outputId": "7a7af3b4-1a07-4fdd-dbdc-5793e1fdc19c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "35/35 [==============================] - 10s 274ms/step - loss: 2.0010 - accuracy: 0.3070 - val_loss: 3.9611 - val_accuracy: 0.1525\n",
            "Epoch 2/200\n",
            "35/35 [==============================] - 6s 177ms/step - loss: 1.9936 - accuracy: 0.3077 - val_loss: 3.9491 - val_accuracy: 0.1489\n",
            "Epoch 3/200\n",
            "35/35 [==============================] - 7s 199ms/step - loss: 1.9839 - accuracy: 0.3122 - val_loss: 3.8818 - val_accuracy: 0.1507\n",
            "Epoch 4/200\n",
            "35/35 [==============================] - 5s 128ms/step - loss: 1.9784 - accuracy: 0.3141 - val_loss: 3.9934 - val_accuracy: 0.1525\n",
            "Epoch 5/200\n",
            "35/35 [==============================] - 5s 149ms/step - loss: 1.9694 - accuracy: 0.3162 - val_loss: 4.0735 - val_accuracy: 0.1512\n",
            "Epoch 6/200\n",
            "35/35 [==============================] - 6s 183ms/step - loss: 1.9601 - accuracy: 0.3234 - val_loss: 4.2488 - val_accuracy: 0.1498\n",
            "Epoch 7/200\n",
            "35/35 [==============================] - 5s 129ms/step - loss: 1.9542 - accuracy: 0.3229 - val_loss: 4.2350 - val_accuracy: 0.1494\n",
            "Epoch 8/200\n",
            "35/35 [==============================] - 5s 141ms/step - loss: 1.9518 - accuracy: 0.3271 - val_loss: 4.2682 - val_accuracy: 0.1534\n",
            "Epoch 9/200\n",
            "35/35 [==============================] - 4s 116ms/step - loss: 1.9471 - accuracy: 0.3294 - val_loss: 4.1957 - val_accuracy: 0.1516\n",
            "Epoch 10/200\n",
            "35/35 [==============================] - 3s 96ms/step - loss: 1.9478 - accuracy: 0.3302 - val_loss: 4.3685 - val_accuracy: 0.1494\n",
            "Epoch 11/200\n",
            "35/35 [==============================] - 5s 127ms/step - loss: 1.9414 - accuracy: 0.3314 - val_loss: 4.4856 - val_accuracy: 0.1543\n",
            "Epoch 12/200\n",
            "35/35 [==============================] - 4s 116ms/step - loss: 1.9324 - accuracy: 0.3383 - val_loss: 4.4945 - val_accuracy: 0.1476\n",
            "Epoch 13/200\n",
            "35/35 [==============================] - 4s 104ms/step - loss: 1.9312 - accuracy: 0.3376 - val_loss: 4.5861 - val_accuracy: 0.1512\n",
            "Epoch 14/200\n",
            "35/35 [==============================] - 3s 88ms/step - loss: 1.9279 - accuracy: 0.3388 - val_loss: 4.3488 - val_accuracy: 0.1485\n",
            "Epoch 15/200\n",
            "35/35 [==============================] - 3s 102ms/step - loss: 1.9235 - accuracy: 0.3428 - val_loss: 4.4146 - val_accuracy: 0.1463\n",
            "Epoch 16/200\n",
            "35/35 [==============================] - 4s 109ms/step - loss: 1.9285 - accuracy: 0.3381 - val_loss: 4.6456 - val_accuracy: 0.1507\n",
            "Epoch 17/200\n",
            "35/35 [==============================] - 2s 69ms/step - loss: 1.9165 - accuracy: 0.3436 - val_loss: 4.5619 - val_accuracy: 0.1534\n",
            "Epoch 18/200\n",
            "35/35 [==============================] - 4s 106ms/step - loss: 1.9640 - accuracy: 0.3376 - val_loss: 4.1834 - val_accuracy: 0.1534\n",
            "Epoch 19/200\n",
            "35/35 [==============================] - 2s 68ms/step - loss: 1.9298 - accuracy: 0.3432 - val_loss: 4.6415 - val_accuracy: 0.1507\n",
            "Epoch 20/200\n",
            "35/35 [==============================] - 2s 67ms/step - loss: 1.9242 - accuracy: 0.3442 - val_loss: 4.5351 - val_accuracy: 0.1543\n",
            "Epoch 21/200\n",
            "35/35 [==============================] - 3s 97ms/step - loss: 1.9408 - accuracy: 0.3364 - val_loss: 4.9699 - val_accuracy: 0.1463\n",
            "Epoch 22/200\n",
            "35/35 [==============================] - 2s 65ms/step - loss: 1.9627 - accuracy: 0.3275 - val_loss: 4.0753 - val_accuracy: 0.1539\n",
            "Epoch 23/200\n",
            "35/35 [==============================] - 3s 74ms/step - loss: 1.9126 - accuracy: 0.3447 - val_loss: 4.7007 - val_accuracy: 0.1507\n",
            "Epoch 24/200\n",
            "35/35 [==============================] - 2s 56ms/step - loss: 1.8933 - accuracy: 0.3556 - val_loss: 4.7949 - val_accuracy: 0.1467\n",
            "Epoch 25/200\n",
            "35/35 [==============================] - 4s 105ms/step - loss: 1.9533 - accuracy: 0.3336 - val_loss: 4.6211 - val_accuracy: 0.1440\n",
            "Epoch 26/200\n",
            "35/35 [==============================] - 3s 81ms/step - loss: 1.9576 - accuracy: 0.3294 - val_loss: 4.7084 - val_accuracy: 0.1427\n",
            "Epoch 27/200\n",
            "35/35 [==============================] - 2s 60ms/step - loss: 1.9387 - accuracy: 0.3405 - val_loss: 4.4554 - val_accuracy: 0.1503\n",
            "Epoch 28/200\n",
            "35/35 [==============================] - 3s 75ms/step - loss: 1.9075 - accuracy: 0.3477 - val_loss: 4.6205 - val_accuracy: 0.1503\n",
            "Epoch 29/200\n",
            "35/35 [==============================] - 2s 65ms/step - loss: 1.9085 - accuracy: 0.3492 - val_loss: 4.5879 - val_accuracy: 0.1489\n",
            "Epoch 30/200\n",
            "35/35 [==============================] - 2s 46ms/step - loss: 1.8948 - accuracy: 0.3540 - val_loss: 4.4672 - val_accuracy: 0.1467\n",
            "Epoch 31/200\n",
            "35/35 [==============================] - 1s 41ms/step - loss: 1.8869 - accuracy: 0.3561 - val_loss: 4.8393 - val_accuracy: 0.1507\n",
            "Epoch 32/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.8828 - accuracy: 0.3577 - val_loss: 4.9822 - val_accuracy: 0.1485\n",
            "Epoch 33/200\n",
            "35/35 [==============================] - 2s 68ms/step - loss: 1.8809 - accuracy: 0.3580 - val_loss: 4.7993 - val_accuracy: 0.1521\n",
            "Epoch 34/200\n",
            "35/35 [==============================] - 2s 64ms/step - loss: 1.8742 - accuracy: 0.3614 - val_loss: 4.7467 - val_accuracy: 0.1539\n",
            "Epoch 35/200\n",
            "35/35 [==============================] - 2s 51ms/step - loss: 1.8785 - accuracy: 0.3578 - val_loss: 4.9498 - val_accuracy: 0.1498\n",
            "Epoch 36/200\n",
            "35/35 [==============================] - 2s 51ms/step - loss: 1.8713 - accuracy: 0.3594 - val_loss: 4.7637 - val_accuracy: 0.1530\n",
            "Epoch 37/200\n",
            "35/35 [==============================] - 2s 52ms/step - loss: 1.8664 - accuracy: 0.3609 - val_loss: 4.8890 - val_accuracy: 0.1476\n",
            "Epoch 38/200\n",
            "35/35 [==============================] - 2s 46ms/step - loss: 1.8679 - accuracy: 0.3618 - val_loss: 4.8002 - val_accuracy: 0.1503\n",
            "Epoch 39/200\n",
            "35/35 [==============================] - 2s 51ms/step - loss: 1.8610 - accuracy: 0.3650 - val_loss: 5.0333 - val_accuracy: 0.1449\n",
            "Epoch 40/200\n",
            "35/35 [==============================] - 3s 74ms/step - loss: 1.8587 - accuracy: 0.3651 - val_loss: 5.0369 - val_accuracy: 0.1472\n",
            "Epoch 41/200\n",
            "35/35 [==============================] - 2s 45ms/step - loss: 1.8588 - accuracy: 0.3645 - val_loss: 4.9516 - val_accuracy: 0.1507\n",
            "Epoch 42/200\n",
            "35/35 [==============================] - 2s 72ms/step - loss: 1.8560 - accuracy: 0.3647 - val_loss: 5.0472 - val_accuracy: 0.1436\n",
            "Epoch 43/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.8541 - accuracy: 0.3677 - val_loss: 5.0590 - val_accuracy: 0.1467\n",
            "Epoch 44/200\n",
            "35/35 [==============================] - 1s 38ms/step - loss: 1.8509 - accuracy: 0.3672 - val_loss: 5.1281 - val_accuracy: 0.1476\n",
            "Epoch 45/200\n",
            "35/35 [==============================] - 2s 46ms/step - loss: 1.8565 - accuracy: 0.3662 - val_loss: 4.7755 - val_accuracy: 0.1476\n",
            "Epoch 46/200\n",
            "35/35 [==============================] - 2s 46ms/step - loss: 1.8775 - accuracy: 0.3578 - val_loss: 4.8269 - val_accuracy: 0.1454\n",
            "Epoch 47/200\n",
            "35/35 [==============================] - 2s 46ms/step - loss: 1.8702 - accuracy: 0.3601 - val_loss: 4.8469 - val_accuracy: 0.1485\n",
            "Epoch 48/200\n",
            "35/35 [==============================] - 2s 43ms/step - loss: 1.8520 - accuracy: 0.3680 - val_loss: 4.9843 - val_accuracy: 0.1543\n",
            "Epoch 49/200\n",
            "35/35 [==============================] - 2s 49ms/step - loss: 1.8494 - accuracy: 0.3685 - val_loss: 5.1076 - val_accuracy: 0.1494\n",
            "Epoch 50/200\n",
            "35/35 [==============================] - 2s 64ms/step - loss: 1.8483 - accuracy: 0.3682 - val_loss: 4.8120 - val_accuracy: 0.1449\n",
            "Epoch 51/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.8472 - accuracy: 0.3667 - val_loss: 4.9406 - val_accuracy: 0.1431\n",
            "Epoch 52/200\n",
            "35/35 [==============================] - 2s 56ms/step - loss: 1.8386 - accuracy: 0.3718 - val_loss: 4.8247 - val_accuracy: 0.1503\n",
            "Epoch 53/200\n",
            "35/35 [==============================] - 2s 60ms/step - loss: 1.8339 - accuracy: 0.3717 - val_loss: 5.1240 - val_accuracy: 0.1436\n",
            "Epoch 54/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.8331 - accuracy: 0.3726 - val_loss: 5.2543 - val_accuracy: 0.1454\n",
            "Epoch 55/200\n",
            "35/35 [==============================] - 2s 47ms/step - loss: 1.8302 - accuracy: 0.3733 - val_loss: 5.0449 - val_accuracy: 0.1489\n",
            "Epoch 56/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.8253 - accuracy: 0.3752 - val_loss: 5.0728 - val_accuracy: 0.1449\n",
            "Epoch 57/200\n",
            "35/35 [==============================] - 2s 49ms/step - loss: 1.8236 - accuracy: 0.3764 - val_loss: 5.1750 - val_accuracy: 0.1395\n",
            "Epoch 58/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.8370 - accuracy: 0.3687 - val_loss: 5.2470 - val_accuracy: 0.1400\n",
            "Epoch 59/200\n",
            "35/35 [==============================] - 2s 52ms/step - loss: 1.8401 - accuracy: 0.3707 - val_loss: 5.1191 - val_accuracy: 0.1512\n",
            "Epoch 60/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.8275 - accuracy: 0.3747 - val_loss: 4.9880 - val_accuracy: 0.1395\n",
            "Epoch 61/200\n",
            "35/35 [==============================] - 2s 46ms/step - loss: 1.8224 - accuracy: 0.3774 - val_loss: 5.2613 - val_accuracy: 0.1431\n",
            "Epoch 62/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.8182 - accuracy: 0.3785 - val_loss: 5.1343 - val_accuracy: 0.1494\n",
            "Epoch 63/200\n",
            "35/35 [==============================] - 1s 41ms/step - loss: 1.8459 - accuracy: 0.3691 - val_loss: 5.0498 - val_accuracy: 0.1498\n",
            "Epoch 64/200\n",
            "35/35 [==============================] - 2s 46ms/step - loss: 1.8620 - accuracy: 0.3664 - val_loss: 4.7076 - val_accuracy: 0.1485\n",
            "Epoch 65/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.8603 - accuracy: 0.3636 - val_loss: 4.7183 - val_accuracy: 0.1454\n",
            "Epoch 66/200\n",
            "35/35 [==============================] - 2s 65ms/step - loss: 1.9018 - accuracy: 0.3502 - val_loss: 4.5385 - val_accuracy: 0.1454\n",
            "Epoch 67/200\n",
            "35/35 [==============================] - 2s 57ms/step - loss: 1.8553 - accuracy: 0.3678 - val_loss: 4.8404 - val_accuracy: 0.1516\n",
            "Epoch 68/200\n",
            "35/35 [==============================] - 2s 50ms/step - loss: 1.8463 - accuracy: 0.3696 - val_loss: 5.0716 - val_accuracy: 0.1440\n",
            "Epoch 69/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.8262 - accuracy: 0.3739 - val_loss: 4.9276 - val_accuracy: 0.1472\n",
            "Epoch 70/200\n",
            "35/35 [==============================] - 2s 46ms/step - loss: 1.8187 - accuracy: 0.3777 - val_loss: 4.9901 - val_accuracy: 0.1431\n",
            "Epoch 71/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.8312 - accuracy: 0.3737 - val_loss: 4.7341 - val_accuracy: 0.1584\n",
            "Epoch 72/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.8249 - accuracy: 0.3766 - val_loss: 5.0386 - val_accuracy: 0.1476\n",
            "Epoch 73/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.8121 - accuracy: 0.3788 - val_loss: 4.9844 - val_accuracy: 0.1458\n",
            "Epoch 74/200\n",
            "35/35 [==============================] - 2s 46ms/step - loss: 1.8505 - accuracy: 0.3712 - val_loss: 5.0502 - val_accuracy: 0.1498\n",
            "Epoch 75/200\n",
            "35/35 [==============================] - 2s 50ms/step - loss: 1.8601 - accuracy: 0.3656 - val_loss: 4.9166 - val_accuracy: 0.1445\n",
            "Epoch 76/200\n",
            "35/35 [==============================] - 2s 44ms/step - loss: 1.8444 - accuracy: 0.3688 - val_loss: 5.0111 - val_accuracy: 0.1472\n",
            "Epoch 77/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.8306 - accuracy: 0.3741 - val_loss: 4.9610 - val_accuracy: 0.1445\n",
            "Epoch 78/200\n",
            "35/35 [==============================] - 2s 46ms/step - loss: 1.8246 - accuracy: 0.3747 - val_loss: 4.9691 - val_accuracy: 0.1458\n",
            "Epoch 79/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.8186 - accuracy: 0.3773 - val_loss: 5.0161 - val_accuracy: 0.1445\n",
            "Epoch 80/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.8142 - accuracy: 0.3776 - val_loss: 5.0876 - val_accuracy: 0.1467\n",
            "Epoch 81/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.8102 - accuracy: 0.3810 - val_loss: 5.2652 - val_accuracy: 0.1436\n",
            "Epoch 82/200\n",
            "35/35 [==============================] - 2s 46ms/step - loss: 1.8000 - accuracy: 0.3841 - val_loss: 5.1597 - val_accuracy: 0.1449\n",
            "Epoch 83/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.7948 - accuracy: 0.3841 - val_loss: 5.1859 - val_accuracy: 0.1449\n",
            "Epoch 84/200\n",
            "35/35 [==============================] - 1s 43ms/step - loss: 1.7953 - accuracy: 0.3843 - val_loss: 5.3085 - val_accuracy: 0.1480\n",
            "Epoch 85/200\n",
            "35/35 [==============================] - 2s 51ms/step - loss: 1.8163 - accuracy: 0.3797 - val_loss: 5.1402 - val_accuracy: 0.1463\n",
            "Epoch 86/200\n",
            "35/35 [==============================] - 2s 49ms/step - loss: 1.7998 - accuracy: 0.3846 - val_loss: 5.1603 - val_accuracy: 0.1449\n",
            "Epoch 87/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.7925 - accuracy: 0.3848 - val_loss: 5.0723 - val_accuracy: 0.1427\n",
            "Epoch 88/200\n",
            "35/35 [==============================] - 1s 41ms/step - loss: 1.7938 - accuracy: 0.3867 - val_loss: 5.1388 - val_accuracy: 0.1427\n",
            "Epoch 89/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.8347 - accuracy: 0.3737 - val_loss: 4.9096 - val_accuracy: 0.1445\n",
            "Epoch 90/200\n",
            "35/35 [==============================] - 2s 46ms/step - loss: 1.8316 - accuracy: 0.3751 - val_loss: 5.0447 - val_accuracy: 0.1467\n",
            "Epoch 91/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.8036 - accuracy: 0.3833 - val_loss: 5.1608 - val_accuracy: 0.1458\n",
            "Epoch 92/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.8291 - accuracy: 0.3788 - val_loss: 5.4388 - val_accuracy: 0.1485\n",
            "Epoch 93/200\n",
            "35/35 [==============================] - 1s 38ms/step - loss: 1.8197 - accuracy: 0.3837 - val_loss: 5.3672 - val_accuracy: 0.1498\n",
            "Epoch 94/200\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 1.8242 - accuracy: 0.3803 - val_loss: 5.3683 - val_accuracy: 0.1445\n",
            "Epoch 95/200\n",
            "35/35 [==============================] - 2s 51ms/step - loss: 1.8506 - accuracy: 0.3750 - val_loss: 4.9038 - val_accuracy: 0.1485\n",
            "Epoch 96/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.8398 - accuracy: 0.3796 - val_loss: 5.6458 - val_accuracy: 0.1480\n",
            "Epoch 97/200\n",
            "35/35 [==============================] - 2s 45ms/step - loss: 1.8325 - accuracy: 0.3824 - val_loss: 4.9991 - val_accuracy: 0.1454\n",
            "Epoch 98/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.8253 - accuracy: 0.3822 - val_loss: 5.2616 - val_accuracy: 0.1445\n",
            "Epoch 99/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.8046 - accuracy: 0.3896 - val_loss: 5.1597 - val_accuracy: 0.1476\n",
            "Epoch 100/200\n",
            "35/35 [==============================] - 2s 47ms/step - loss: 1.7933 - accuracy: 0.3927 - val_loss: 5.5218 - val_accuracy: 0.1476\n",
            "Epoch 101/200\n",
            "35/35 [==============================] - 2s 42ms/step - loss: 1.7953 - accuracy: 0.3903 - val_loss: 5.0957 - val_accuracy: 0.1458\n",
            "Epoch 102/200\n",
            "35/35 [==============================] - 1s 38ms/step - loss: 1.7972 - accuracy: 0.3879 - val_loss: 5.3703 - val_accuracy: 0.1436\n",
            "Epoch 103/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.8503 - accuracy: 0.3765 - val_loss: 4.2789 - val_accuracy: 0.1422\n",
            "Epoch 104/200\n",
            "35/35 [==============================] - 2s 49ms/step - loss: 1.8973 - accuracy: 0.3580 - val_loss: 4.5126 - val_accuracy: 0.1503\n",
            "Epoch 105/200\n",
            "35/35 [==============================] - 2s 50ms/step - loss: 1.8712 - accuracy: 0.3690 - val_loss: 4.8386 - val_accuracy: 0.1507\n",
            "Epoch 106/200\n",
            "35/35 [==============================] - 2s 45ms/step - loss: 1.8168 - accuracy: 0.3824 - val_loss: 4.9332 - val_accuracy: 0.1467\n",
            "Epoch 107/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.9048 - accuracy: 0.3596 - val_loss: 3.8085 - val_accuracy: 0.1503\n",
            "Epoch 108/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.9815 - accuracy: 0.3472 - val_loss: 4.4585 - val_accuracy: 0.1476\n",
            "Epoch 109/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.9233 - accuracy: 0.3701 - val_loss: 4.7161 - val_accuracy: 0.1476\n",
            "Epoch 110/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.8936 - accuracy: 0.3694 - val_loss: 5.2397 - val_accuracy: 0.1530\n",
            "Epoch 111/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.8767 - accuracy: 0.3756 - val_loss: 5.3122 - val_accuracy: 0.1467\n",
            "Epoch 112/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.8622 - accuracy: 0.3791 - val_loss: 5.1019 - val_accuracy: 0.1503\n",
            "Epoch 113/200\n",
            "35/35 [==============================] - 1s 43ms/step - loss: 1.8447 - accuracy: 0.3809 - val_loss: 5.2961 - val_accuracy: 0.1440\n",
            "Epoch 114/200\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 1.8422 - accuracy: 0.3842 - val_loss: 5.2949 - val_accuracy: 0.1449\n",
            "Epoch 115/200\n",
            "35/35 [==============================] - 2s 51ms/step - loss: 1.8317 - accuracy: 0.3856 - val_loss: 5.2933 - val_accuracy: 0.1418\n",
            "Epoch 116/200\n",
            "35/35 [==============================] - 2s 45ms/step - loss: 1.8199 - accuracy: 0.3898 - val_loss: 5.3501 - val_accuracy: 0.1440\n",
            "Epoch 117/200\n",
            "35/35 [==============================] - 2s 46ms/step - loss: 1.8122 - accuracy: 0.3911 - val_loss: 5.2933 - val_accuracy: 0.1458\n",
            "Epoch 118/200\n",
            "35/35 [==============================] - 2s 48ms/step - loss: 1.8071 - accuracy: 0.3975 - val_loss: 5.0880 - val_accuracy: 0.1422\n",
            "Epoch 119/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.7997 - accuracy: 0.4040 - val_loss: 5.2355 - val_accuracy: 0.1431\n",
            "Epoch 120/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.7930 - accuracy: 0.4078 - val_loss: 5.2427 - val_accuracy: 0.1489\n",
            "Epoch 121/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.7861 - accuracy: 0.4110 - val_loss: 5.2675 - val_accuracy: 0.1418\n",
            "Epoch 122/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.7831 - accuracy: 0.4155 - val_loss: 5.4731 - val_accuracy: 0.1409\n",
            "Epoch 123/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.7772 - accuracy: 0.4171 - val_loss: 5.3583 - val_accuracy: 0.1422\n",
            "Epoch 124/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.7685 - accuracy: 0.4215 - val_loss: 5.4090 - val_accuracy: 0.1404\n",
            "Epoch 125/200\n",
            "35/35 [==============================] - 2s 49ms/step - loss: 1.7809 - accuracy: 0.4182 - val_loss: 5.3583 - val_accuracy: 0.1377\n",
            "Epoch 126/200\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 1.7710 - accuracy: 0.4235 - val_loss: 5.3887 - val_accuracy: 0.1368\n",
            "Epoch 127/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.7711 - accuracy: 0.4210 - val_loss: 5.4132 - val_accuracy: 0.1422\n",
            "Epoch 128/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.7764 - accuracy: 0.4195 - val_loss: 5.2836 - val_accuracy: 0.1382\n",
            "Epoch 129/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.7807 - accuracy: 0.4176 - val_loss: 5.0086 - val_accuracy: 0.1409\n",
            "Epoch 130/200\n",
            "35/35 [==============================] - 2s 47ms/step - loss: 1.7740 - accuracy: 0.4217 - val_loss: 5.4989 - val_accuracy: 0.1418\n",
            "Epoch 131/200\n",
            "35/35 [==============================] - 1s 41ms/step - loss: 1.7529 - accuracy: 0.4308 - val_loss: 5.3459 - val_accuracy: 0.1377\n",
            "Epoch 132/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.7453 - accuracy: 0.4329 - val_loss: 5.5080 - val_accuracy: 0.1454\n",
            "Epoch 133/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.7379 - accuracy: 0.4353 - val_loss: 5.5395 - val_accuracy: 0.1395\n",
            "Epoch 134/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.7438 - accuracy: 0.4329 - val_loss: 5.4581 - val_accuracy: 0.1431\n",
            "Epoch 135/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.7501 - accuracy: 0.4317 - val_loss: 5.2916 - val_accuracy: 0.1454\n",
            "Epoch 136/200\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 1.7487 - accuracy: 0.4303 - val_loss: 5.3536 - val_accuracy: 0.1391\n",
            "Epoch 137/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.7376 - accuracy: 0.4371 - val_loss: 5.3214 - val_accuracy: 0.1413\n",
            "Epoch 138/200\n",
            "35/35 [==============================] - 2s 47ms/step - loss: 1.7283 - accuracy: 0.4400 - val_loss: 5.4176 - val_accuracy: 0.1422\n",
            "Epoch 139/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.7232 - accuracy: 0.4403 - val_loss: 5.3465 - val_accuracy: 0.1413\n",
            "Epoch 140/200\n",
            "35/35 [==============================] - 2s 47ms/step - loss: 1.7291 - accuracy: 0.4391 - val_loss: 5.3523 - val_accuracy: 0.1377\n",
            "Epoch 141/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.7214 - accuracy: 0.4439 - val_loss: 5.6601 - val_accuracy: 0.1350\n",
            "Epoch 142/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.7750 - accuracy: 0.4271 - val_loss: 5.8602 - val_accuracy: 0.1418\n",
            "Epoch 143/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.7264 - accuracy: 0.4404 - val_loss: 5.7597 - val_accuracy: 0.1359\n",
            "Epoch 144/200\n",
            "35/35 [==============================] - 1s 41ms/step - loss: 1.7303 - accuracy: 0.4385 - val_loss: 5.6843 - val_accuracy: 0.1395\n",
            "Epoch 145/200\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 1.7910 - accuracy: 0.4176 - val_loss: 5.1390 - val_accuracy: 0.1395\n",
            "Epoch 146/200\n",
            "35/35 [==============================] - 2s 50ms/step - loss: 1.7438 - accuracy: 0.4340 - val_loss: 5.0656 - val_accuracy: 0.1472\n",
            "Epoch 147/200\n",
            "35/35 [==============================] - 2s 47ms/step - loss: 1.7512 - accuracy: 0.4284 - val_loss: 5.1666 - val_accuracy: 0.1418\n",
            "Epoch 148/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.7313 - accuracy: 0.4384 - val_loss: 5.3877 - val_accuracy: 0.1355\n",
            "Epoch 149/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.7201 - accuracy: 0.4405 - val_loss: 5.6080 - val_accuracy: 0.1395\n",
            "Epoch 150/200\n",
            "35/35 [==============================] - 1s 41ms/step - loss: 1.7169 - accuracy: 0.4417 - val_loss: 5.3906 - val_accuracy: 0.1377\n",
            "Epoch 151/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.7093 - accuracy: 0.4429 - val_loss: 5.5876 - val_accuracy: 0.1373\n",
            "Epoch 152/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.7222 - accuracy: 0.4420 - val_loss: 5.4392 - val_accuracy: 0.1404\n",
            "Epoch 153/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.7145 - accuracy: 0.4414 - val_loss: 5.5306 - val_accuracy: 0.1395\n",
            "Epoch 154/200\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 1.7080 - accuracy: 0.4464 - val_loss: 5.9267 - val_accuracy: 0.1445\n",
            "Epoch 155/200\n",
            "35/35 [==============================] - 1s 43ms/step - loss: 1.7055 - accuracy: 0.4463 - val_loss: 5.7532 - val_accuracy: 0.1395\n",
            "Epoch 156/200\n",
            "35/35 [==============================] - 2s 48ms/step - loss: 1.6927 - accuracy: 0.4504 - val_loss: 5.6591 - val_accuracy: 0.1436\n",
            "Epoch 157/200\n",
            "35/35 [==============================] - 2s 56ms/step - loss: 1.6832 - accuracy: 0.4539 - val_loss: 5.7430 - val_accuracy: 0.1418\n",
            "Epoch 158/200\n",
            "35/35 [==============================] - 2s 51ms/step - loss: 1.6823 - accuracy: 0.4526 - val_loss: 5.6982 - val_accuracy: 0.1400\n",
            "Epoch 159/200\n",
            "35/35 [==============================] - 1s 39ms/step - loss: 1.6774 - accuracy: 0.4522 - val_loss: 5.7209 - val_accuracy: 0.1418\n",
            "Epoch 160/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.6914 - accuracy: 0.4485 - val_loss: 5.6640 - val_accuracy: 0.1422\n",
            "Epoch 161/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.6937 - accuracy: 0.4483 - val_loss: 5.8087 - val_accuracy: 0.1409\n",
            "Epoch 162/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.6924 - accuracy: 0.4480 - val_loss: 6.0156 - val_accuracy: 0.1413\n",
            "Epoch 163/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.7169 - accuracy: 0.4438 - val_loss: 5.5298 - val_accuracy: 0.1445\n",
            "Epoch 164/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.7463 - accuracy: 0.4315 - val_loss: 5.4184 - val_accuracy: 0.1395\n",
            "Epoch 165/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.7614 - accuracy: 0.4336 - val_loss: 4.4132 - val_accuracy: 0.1404\n",
            "Epoch 166/200\n",
            "35/35 [==============================] - 2s 50ms/step - loss: 1.7956 - accuracy: 0.4389 - val_loss: 5.1829 - val_accuracy: 0.1359\n",
            "Epoch 167/200\n",
            "35/35 [==============================] - 1s 41ms/step - loss: 1.7543 - accuracy: 0.4473 - val_loss: 5.2196 - val_accuracy: 0.1373\n",
            "Epoch 168/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.7416 - accuracy: 0.4481 - val_loss: 5.6059 - val_accuracy: 0.1413\n",
            "Epoch 169/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.7226 - accuracy: 0.4545 - val_loss: 5.5470 - val_accuracy: 0.1355\n",
            "Epoch 170/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.7111 - accuracy: 0.4546 - val_loss: 5.5441 - val_accuracy: 0.1422\n",
            "Epoch 171/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.6982 - accuracy: 0.4585 - val_loss: 5.9718 - val_accuracy: 0.1382\n",
            "Epoch 172/200\n",
            "35/35 [==============================] - 1s 41ms/step - loss: 1.6884 - accuracy: 0.4598 - val_loss: 6.3667 - val_accuracy: 0.1373\n",
            "Epoch 173/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.6716 - accuracy: 0.4625 - val_loss: 6.3149 - val_accuracy: 0.1310\n",
            "Epoch 174/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.6623 - accuracy: 0.4630 - val_loss: 6.2105 - val_accuracy: 0.1332\n",
            "Epoch 175/200\n",
            "35/35 [==============================] - 2s 48ms/step - loss: 1.6592 - accuracy: 0.4652 - val_loss: 5.8923 - val_accuracy: 0.1292\n",
            "Epoch 176/200\n",
            "35/35 [==============================] - 2s 51ms/step - loss: 1.6629 - accuracy: 0.4622 - val_loss: 5.9190 - val_accuracy: 0.1359\n",
            "Epoch 177/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.6596 - accuracy: 0.4649 - val_loss: 5.9480 - val_accuracy: 0.1301\n",
            "Epoch 178/200\n",
            "35/35 [==============================] - 2s 46ms/step - loss: 1.6619 - accuracy: 0.4620 - val_loss: 5.8557 - val_accuracy: 0.1400\n",
            "Epoch 179/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.6562 - accuracy: 0.4653 - val_loss: 5.8519 - val_accuracy: 0.1332\n",
            "Epoch 180/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.6590 - accuracy: 0.4653 - val_loss: 5.9108 - val_accuracy: 0.1368\n",
            "Epoch 181/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.6589 - accuracy: 0.4620 - val_loss: 6.0385 - val_accuracy: 0.1359\n",
            "Epoch 182/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.6716 - accuracy: 0.4592 - val_loss: 5.8722 - val_accuracy: 0.1377\n",
            "Epoch 183/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.6513 - accuracy: 0.4667 - val_loss: 5.9188 - val_accuracy: 0.1391\n",
            "Epoch 184/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.6394 - accuracy: 0.4700 - val_loss: 5.9214 - val_accuracy: 0.1355\n",
            "Epoch 185/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.6392 - accuracy: 0.4704 - val_loss: 6.0733 - val_accuracy: 0.1436\n",
            "Epoch 186/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.6406 - accuracy: 0.4689 - val_loss: 5.9719 - val_accuracy: 0.1368\n",
            "Epoch 187/200\n",
            "35/35 [==============================] - 2s 52ms/step - loss: 1.6492 - accuracy: 0.4630 - val_loss: 5.9249 - val_accuracy: 0.1301\n",
            "Epoch 188/200\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 1.6600 - accuracy: 0.4621 - val_loss: 5.8153 - val_accuracy: 0.1252\n",
            "Epoch 189/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.6418 - accuracy: 0.4667 - val_loss: 6.0118 - val_accuracy: 0.1373\n",
            "Epoch 190/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.6389 - accuracy: 0.4690 - val_loss: 5.8911 - val_accuracy: 0.1328\n",
            "Epoch 191/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.6370 - accuracy: 0.4693 - val_loss: 5.8080 - val_accuracy: 0.1373\n",
            "Epoch 192/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.6426 - accuracy: 0.4679 - val_loss: 5.8628 - val_accuracy: 0.1436\n",
            "Epoch 193/200\n",
            "35/35 [==============================] - 1s 38ms/step - loss: 1.6301 - accuracy: 0.4721 - val_loss: 5.8457 - val_accuracy: 0.1445\n",
            "Epoch 194/200\n",
            "35/35 [==============================] - 1s 41ms/step - loss: 1.6286 - accuracy: 0.4723 - val_loss: 5.9989 - val_accuracy: 0.1274\n",
            "Epoch 195/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.6433 - accuracy: 0.4669 - val_loss: 5.8569 - val_accuracy: 0.1391\n",
            "Epoch 196/200\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 1.6484 - accuracy: 0.4666 - val_loss: 5.9612 - val_accuracy: 0.1323\n",
            "Epoch 197/200\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 1.6568 - accuracy: 0.4643 - val_loss: 5.5393 - val_accuracy: 0.1400\n",
            "Epoch 198/200\n",
            "35/35 [==============================] - 1s 36ms/step - loss: 1.6417 - accuracy: 0.4686 - val_loss: 5.9950 - val_accuracy: 0.1391\n",
            "Epoch 199/200\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 1.6288 - accuracy: 0.4721 - val_loss: 5.9566 - val_accuracy: 0.1377\n",
            "Epoch 200/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.6188 - accuracy: 0.4762 - val_loss: 6.1515 - val_accuracy: 0.1341\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7af5ac25b430>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_aug_pad, y_train_aug_cat, epochs=200, batch_size=512, validation_data=(X_test_pad, y_test_cat))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FOnw5KyQ2iS",
        "outputId": "1077b8f1-0f32-45e4-aaea-7ac0b52ec709"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "35/35 [==============================] - 2s 59ms/step - loss: 1.6358 - accuracy: 0.4713 - val_loss: 5.8564 - val_accuracy: 0.1377\n",
            "Epoch 2/200\n",
            "35/35 [==============================] - 2s 44ms/step - loss: 1.6304 - accuracy: 0.4721 - val_loss: 5.9017 - val_accuracy: 0.1409\n",
            "Epoch 3/200\n",
            "35/35 [==============================] - 1s 31ms/step - loss: 1.6256 - accuracy: 0.4749 - val_loss: 5.8220 - val_accuracy: 0.1440\n",
            "Epoch 4/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.6101 - accuracy: 0.4794 - val_loss: 6.1833 - val_accuracy: 0.1377\n",
            "Epoch 5/200\n",
            "35/35 [==============================] - 1s 31ms/step - loss: 1.6068 - accuracy: 0.4785 - val_loss: 6.0880 - val_accuracy: 0.1368\n",
            "Epoch 6/200\n",
            "35/35 [==============================] - 1s 38ms/step - loss: 1.6035 - accuracy: 0.4813 - val_loss: 6.0360 - val_accuracy: 0.1454\n",
            "Epoch 7/200\n",
            "35/35 [==============================] - 2s 44ms/step - loss: 1.5979 - accuracy: 0.4846 - val_loss: 6.2412 - val_accuracy: 0.1431\n",
            "Epoch 8/200\n",
            "35/35 [==============================] - 2s 58ms/step - loss: 1.6090 - accuracy: 0.4781 - val_loss: 5.9917 - val_accuracy: 0.1400\n",
            "Epoch 9/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.6241 - accuracy: 0.4736 - val_loss: 5.9111 - val_accuracy: 0.1418\n",
            "Epoch 10/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.6149 - accuracy: 0.4780 - val_loss: 6.0065 - val_accuracy: 0.1350\n",
            "Epoch 11/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.6208 - accuracy: 0.4747 - val_loss: 6.0601 - val_accuracy: 0.1373\n",
            "Epoch 12/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.6216 - accuracy: 0.4764 - val_loss: 5.8094 - val_accuracy: 0.1373\n",
            "Epoch 13/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.6108 - accuracy: 0.4796 - val_loss: 6.1201 - val_accuracy: 0.1395\n",
            "Epoch 14/200\n",
            "35/35 [==============================] - 1s 36ms/step - loss: 1.6379 - accuracy: 0.4712 - val_loss: 6.0190 - val_accuracy: 0.1431\n",
            "Epoch 15/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.6093 - accuracy: 0.4789 - val_loss: 6.1876 - val_accuracy: 0.1382\n",
            "Epoch 16/200\n",
            "35/35 [==============================] - 2s 45ms/step - loss: 1.5980 - accuracy: 0.4844 - val_loss: 6.0618 - val_accuracy: 0.1391\n",
            "Epoch 17/200\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 1.5929 - accuracy: 0.4855 - val_loss: 6.1251 - val_accuracy: 0.1395\n",
            "Epoch 18/200\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 1.5880 - accuracy: 0.4877 - val_loss: 6.2279 - val_accuracy: 0.1341\n",
            "Epoch 19/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.5915 - accuracy: 0.4867 - val_loss: 6.3871 - val_accuracy: 0.1350\n",
            "Epoch 20/200\n",
            "35/35 [==============================] - 1s 36ms/step - loss: 1.5922 - accuracy: 0.4852 - val_loss: 5.7708 - val_accuracy: 0.1328\n",
            "Epoch 21/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.5956 - accuracy: 0.4837 - val_loss: 6.2797 - val_accuracy: 0.1310\n",
            "Epoch 22/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.5951 - accuracy: 0.4830 - val_loss: 6.1369 - val_accuracy: 0.1328\n",
            "Epoch 23/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.5982 - accuracy: 0.4858 - val_loss: 6.4786 - val_accuracy: 0.1256\n",
            "Epoch 24/200\n",
            "35/35 [==============================] - 1s 36ms/step - loss: 1.6082 - accuracy: 0.4809 - val_loss: 5.8309 - val_accuracy: 0.1314\n",
            "Epoch 25/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.6156 - accuracy: 0.4783 - val_loss: 6.0720 - val_accuracy: 0.1323\n",
            "Epoch 26/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.6318 - accuracy: 0.4715 - val_loss: 5.8581 - val_accuracy: 0.1359\n",
            "Epoch 27/200\n",
            "35/35 [==============================] - 1s 38ms/step - loss: 1.6228 - accuracy: 0.4753 - val_loss: 6.0460 - val_accuracy: 0.1364\n",
            "Epoch 28/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.6502 - accuracy: 0.4681 - val_loss: 6.0976 - val_accuracy: 0.1400\n",
            "Epoch 29/200\n",
            "35/35 [==============================] - 2s 58ms/step - loss: 1.6347 - accuracy: 0.4731 - val_loss: 6.0253 - val_accuracy: 0.1346\n",
            "Epoch 30/200\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 1.6279 - accuracy: 0.4749 - val_loss: 5.7044 - val_accuracy: 0.1400\n",
            "Epoch 31/200\n",
            "35/35 [==============================] - 1s 38ms/step - loss: 1.6178 - accuracy: 0.4766 - val_loss: 6.1733 - val_accuracy: 0.1355\n",
            "Epoch 32/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.5953 - accuracy: 0.4865 - val_loss: 6.1810 - val_accuracy: 0.1418\n",
            "Epoch 33/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.5794 - accuracy: 0.4909 - val_loss: 6.1063 - val_accuracy: 0.1377\n",
            "Epoch 34/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.5994 - accuracy: 0.4840 - val_loss: 6.0013 - val_accuracy: 0.1400\n",
            "Epoch 35/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.6267 - accuracy: 0.4759 - val_loss: 5.7225 - val_accuracy: 0.1404\n",
            "Epoch 36/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.5956 - accuracy: 0.4855 - val_loss: 5.9985 - val_accuracy: 0.1377\n",
            "Epoch 37/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.5789 - accuracy: 0.4910 - val_loss: 6.2664 - val_accuracy: 0.1373\n",
            "Epoch 38/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.5716 - accuracy: 0.4927 - val_loss: 6.2602 - val_accuracy: 0.1346\n",
            "Epoch 39/200\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 1.5644 - accuracy: 0.4953 - val_loss: 6.3635 - val_accuracy: 0.1373\n",
            "Epoch 40/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.5624 - accuracy: 0.4955 - val_loss: 6.3975 - val_accuracy: 0.1373\n",
            "Epoch 41/200\n",
            "35/35 [==============================] - 1s 39ms/step - loss: 1.5610 - accuracy: 0.4964 - val_loss: 6.2431 - val_accuracy: 0.1332\n",
            "Epoch 42/200\n",
            "35/35 [==============================] - 1s 36ms/step - loss: 1.5614 - accuracy: 0.4969 - val_loss: 6.5188 - val_accuracy: 0.1373\n",
            "Epoch 43/200\n",
            "35/35 [==============================] - 1s 36ms/step - loss: 1.5787 - accuracy: 0.4893 - val_loss: 6.2607 - val_accuracy: 0.1391\n",
            "Epoch 44/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.5666 - accuracy: 0.4946 - val_loss: 6.4338 - val_accuracy: 0.1386\n",
            "Epoch 45/200\n",
            "35/35 [==============================] - 1s 36ms/step - loss: 1.5540 - accuracy: 0.4980 - val_loss: 6.5102 - val_accuracy: 0.1386\n",
            "Epoch 46/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.5607 - accuracy: 0.4970 - val_loss: 6.5568 - val_accuracy: 0.1418\n",
            "Epoch 47/200\n",
            "35/35 [==============================] - 1s 36ms/step - loss: 1.5563 - accuracy: 0.4993 - val_loss: 6.3445 - val_accuracy: 0.1427\n",
            "Epoch 48/200\n",
            "35/35 [==============================] - 1s 36ms/step - loss: 1.5669 - accuracy: 0.4929 - val_loss: 6.3599 - val_accuracy: 0.1400\n",
            "Epoch 49/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.5629 - accuracy: 0.4962 - val_loss: 6.3274 - val_accuracy: 0.1364\n",
            "Epoch 50/200\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 1.5595 - accuracy: 0.4974 - val_loss: 6.2423 - val_accuracy: 0.1323\n",
            "Epoch 51/200\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 1.5654 - accuracy: 0.4912 - val_loss: 6.5264 - val_accuracy: 0.1386\n",
            "Epoch 52/200\n",
            "35/35 [==============================] - 1s 36ms/step - loss: 1.5526 - accuracy: 0.4991 - val_loss: 6.6466 - val_accuracy: 0.1279\n",
            "Epoch 53/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.5737 - accuracy: 0.4921 - val_loss: 6.6575 - val_accuracy: 0.1409\n",
            "Epoch 54/200\n",
            "35/35 [==============================] - 1s 36ms/step - loss: 1.6173 - accuracy: 0.4789 - val_loss: 6.0399 - val_accuracy: 0.1386\n",
            "Epoch 55/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.5882 - accuracy: 0.4878 - val_loss: 6.2436 - val_accuracy: 0.1350\n",
            "Epoch 56/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.5601 - accuracy: 0.4973 - val_loss: 6.5252 - val_accuracy: 0.1373\n",
            "Epoch 57/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.5481 - accuracy: 0.5016 - val_loss: 6.3982 - val_accuracy: 0.1319\n",
            "Epoch 58/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.5368 - accuracy: 0.5049 - val_loss: 6.4900 - val_accuracy: 0.1332\n",
            "Epoch 59/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.5293 - accuracy: 0.5080 - val_loss: 6.6315 - val_accuracy: 0.1332\n",
            "Epoch 60/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.5466 - accuracy: 0.5011 - val_loss: 6.7703 - val_accuracy: 0.1364\n",
            "Epoch 61/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.5446 - accuracy: 0.4999 - val_loss: 6.5199 - val_accuracy: 0.1310\n",
            "Epoch 62/200\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 1.5440 - accuracy: 0.5033 - val_loss: 6.4242 - val_accuracy: 0.1418\n",
            "Epoch 63/200\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 1.5520 - accuracy: 0.4993 - val_loss: 6.6552 - val_accuracy: 0.1306\n",
            "Epoch 64/200\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 1.5454 - accuracy: 0.5025 - val_loss: 6.4742 - val_accuracy: 0.1314\n",
            "Epoch 65/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.5327 - accuracy: 0.5097 - val_loss: 6.5220 - val_accuracy: 0.1314\n",
            "Epoch 66/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.5304 - accuracy: 0.5087 - val_loss: 6.4356 - val_accuracy: 0.1346\n",
            "Epoch 67/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.5298 - accuracy: 0.5073 - val_loss: 6.6279 - val_accuracy: 0.1377\n",
            "Epoch 68/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.5399 - accuracy: 0.5032 - val_loss: 6.4962 - val_accuracy: 0.1382\n",
            "Epoch 69/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.5483 - accuracy: 0.5012 - val_loss: 6.4573 - val_accuracy: 0.1323\n",
            "Epoch 70/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.5297 - accuracy: 0.5063 - val_loss: 6.7019 - val_accuracy: 0.1400\n",
            "Epoch 71/200\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 1.5326 - accuracy: 0.5053 - val_loss: 6.6764 - val_accuracy: 0.1310\n",
            "Epoch 72/200\n",
            "35/35 [==============================] - 2s 45ms/step - loss: 1.5371 - accuracy: 0.5072 - val_loss: 6.7226 - val_accuracy: 0.1395\n",
            "Epoch 73/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.5228 - accuracy: 0.5116 - val_loss: 6.6360 - val_accuracy: 0.1404\n",
            "Epoch 74/200\n",
            "35/35 [==============================] - 2s 48ms/step - loss: 1.5167 - accuracy: 0.5126 - val_loss: 6.7517 - val_accuracy: 0.1297\n",
            "Epoch 75/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.5129 - accuracy: 0.5136 - val_loss: 6.9409 - val_accuracy: 0.1332\n",
            "Epoch 76/200\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 1.5085 - accuracy: 0.5132 - val_loss: 6.8202 - val_accuracy: 0.1319\n",
            "Epoch 77/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.5109 - accuracy: 0.5125 - val_loss: 6.7326 - val_accuracy: 0.1368\n",
            "Epoch 78/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.5125 - accuracy: 0.5130 - val_loss: 6.8831 - val_accuracy: 0.1395\n",
            "Epoch 79/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.5015 - accuracy: 0.5183 - val_loss: 6.7446 - val_accuracy: 0.1364\n",
            "Epoch 80/200\n",
            "35/35 [==============================] - 1s 41ms/step - loss: 1.5007 - accuracy: 0.5173 - val_loss: 7.0205 - val_accuracy: 0.1382\n",
            "Epoch 81/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.5135 - accuracy: 0.5135 - val_loss: 6.8368 - val_accuracy: 0.1355\n",
            "Epoch 82/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.5055 - accuracy: 0.5154 - val_loss: 6.6604 - val_accuracy: 0.1346\n",
            "Epoch 83/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.5017 - accuracy: 0.5172 - val_loss: 6.9106 - val_accuracy: 0.1337\n",
            "Epoch 84/200\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 1.4992 - accuracy: 0.5182 - val_loss: 7.0031 - val_accuracy: 0.1332\n",
            "Epoch 85/200\n",
            "35/35 [==============================] - 1s 43ms/step - loss: 1.5042 - accuracy: 0.5174 - val_loss: 6.9473 - val_accuracy: 0.1400\n",
            "Epoch 86/200\n",
            "35/35 [==============================] - 2s 44ms/step - loss: 1.5613 - accuracy: 0.5006 - val_loss: 6.3568 - val_accuracy: 0.1261\n",
            "Epoch 87/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.6337 - accuracy: 0.4824 - val_loss: 5.6150 - val_accuracy: 0.1283\n",
            "Epoch 88/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.6567 - accuracy: 0.4720 - val_loss: 6.2410 - val_accuracy: 0.1355\n",
            "Epoch 89/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.5739 - accuracy: 0.4978 - val_loss: 6.0335 - val_accuracy: 0.1310\n",
            "Epoch 90/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.5672 - accuracy: 0.5003 - val_loss: 6.1829 - val_accuracy: 0.1382\n",
            "Epoch 91/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.5304 - accuracy: 0.5107 - val_loss: 6.4640 - val_accuracy: 0.1314\n",
            "Epoch 92/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.5092 - accuracy: 0.5162 - val_loss: 6.7455 - val_accuracy: 0.1314\n",
            "Epoch 93/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.5009 - accuracy: 0.5213 - val_loss: 6.7820 - val_accuracy: 0.1319\n",
            "Epoch 94/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.4910 - accuracy: 0.5229 - val_loss: 6.8467 - val_accuracy: 0.1270\n",
            "Epoch 95/200\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 1.4960 - accuracy: 0.5198 - val_loss: 6.9488 - val_accuracy: 0.1328\n",
            "Epoch 96/200\n",
            "35/35 [==============================] - 1s 41ms/step - loss: 1.4952 - accuracy: 0.5208 - val_loss: 6.8058 - val_accuracy: 0.1306\n",
            "Epoch 97/200\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 1.4895 - accuracy: 0.5246 - val_loss: 6.9238 - val_accuracy: 0.1292\n",
            "Epoch 98/200\n",
            "35/35 [==============================] - 1s 43ms/step - loss: 1.4862 - accuracy: 0.5239 - val_loss: 6.8398 - val_accuracy: 0.1288\n",
            "Epoch 99/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.5012 - accuracy: 0.5163 - val_loss: 6.5238 - val_accuracy: 0.1319\n",
            "Epoch 100/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.5090 - accuracy: 0.5176 - val_loss: 7.0881 - val_accuracy: 0.1314\n",
            "Epoch 101/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.5101 - accuracy: 0.5189 - val_loss: 6.7506 - val_accuracy: 0.1337\n",
            "Epoch 102/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.4937 - accuracy: 0.5218 - val_loss: 6.7957 - val_accuracy: 0.1337\n",
            "Epoch 103/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.4957 - accuracy: 0.5210 - val_loss: 6.7481 - val_accuracy: 0.1319\n",
            "Epoch 104/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.4768 - accuracy: 0.5282 - val_loss: 6.8997 - val_accuracy: 0.1310\n",
            "Epoch 105/200\n",
            "35/35 [==============================] - 1s 36ms/step - loss: 1.4753 - accuracy: 0.5261 - val_loss: 6.9846 - val_accuracy: 0.1314\n",
            "Epoch 106/200\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 1.4824 - accuracy: 0.5256 - val_loss: 6.9467 - val_accuracy: 0.1301\n",
            "Epoch 107/200\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 1.4867 - accuracy: 0.5243 - val_loss: 7.1515 - val_accuracy: 0.1256\n",
            "Epoch 108/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.4872 - accuracy: 0.5255 - val_loss: 7.0774 - val_accuracy: 0.1265\n",
            "Epoch 109/200\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 1.4812 - accuracy: 0.5264 - val_loss: 6.8257 - val_accuracy: 0.1337\n",
            "Epoch 110/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.5072 - accuracy: 0.5190 - val_loss: 6.8874 - val_accuracy: 0.1364\n",
            "Epoch 111/200\n",
            "35/35 [==============================] - 1s 36ms/step - loss: 1.5502 - accuracy: 0.5027 - val_loss: 6.5877 - val_accuracy: 0.1373\n",
            "Epoch 112/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.5155 - accuracy: 0.5179 - val_loss: 6.5420 - val_accuracy: 0.1323\n",
            "Epoch 113/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.4997 - accuracy: 0.5223 - val_loss: 6.5737 - val_accuracy: 0.1341\n",
            "Epoch 114/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.5036 - accuracy: 0.5210 - val_loss: 6.7768 - val_accuracy: 0.1297\n",
            "Epoch 115/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.5282 - accuracy: 0.5085 - val_loss: 6.9840 - val_accuracy: 0.1310\n",
            "Epoch 116/200\n",
            "35/35 [==============================] - 1s 36ms/step - loss: 1.4851 - accuracy: 0.5238 - val_loss: 6.8035 - val_accuracy: 0.1310\n",
            "Epoch 117/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.4697 - accuracy: 0.5317 - val_loss: 6.9447 - val_accuracy: 0.1256\n",
            "Epoch 118/200\n",
            "35/35 [==============================] - 2s 43ms/step - loss: 1.4688 - accuracy: 0.5298 - val_loss: 7.1241 - val_accuracy: 0.1256\n",
            "Epoch 119/200\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 1.4628 - accuracy: 0.5343 - val_loss: 7.0631 - val_accuracy: 0.1288\n",
            "Epoch 120/200\n",
            "35/35 [==============================] - 1s 43ms/step - loss: 1.4760 - accuracy: 0.5307 - val_loss: 7.1641 - val_accuracy: 0.1328\n",
            "Epoch 121/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.4833 - accuracy: 0.5263 - val_loss: 7.1746 - val_accuracy: 0.1292\n",
            "Epoch 122/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.4710 - accuracy: 0.5305 - val_loss: 7.2863 - val_accuracy: 0.1301\n",
            "Epoch 123/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.4624 - accuracy: 0.5347 - val_loss: 7.0459 - val_accuracy: 0.1323\n",
            "Epoch 124/200\n",
            "35/35 [==============================] - 1s 38ms/step - loss: 1.4570 - accuracy: 0.5362 - val_loss: 7.2567 - val_accuracy: 0.1328\n",
            "Epoch 125/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.4748 - accuracy: 0.5294 - val_loss: 7.3625 - val_accuracy: 0.1256\n",
            "Epoch 126/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.4699 - accuracy: 0.5287 - val_loss: 7.2787 - val_accuracy: 0.1279\n",
            "Epoch 127/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.5004 - accuracy: 0.5223 - val_loss: 6.6935 - val_accuracy: 0.1350\n",
            "Epoch 128/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.5249 - accuracy: 0.5096 - val_loss: 6.8194 - val_accuracy: 0.1279\n",
            "Epoch 129/200\n",
            "35/35 [==============================] - 1s 39ms/step - loss: 1.4969 - accuracy: 0.5206 - val_loss: 7.0265 - val_accuracy: 0.1252\n",
            "Epoch 130/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.4723 - accuracy: 0.5292 - val_loss: 7.0734 - val_accuracy: 0.1270\n",
            "Epoch 131/200\n",
            "35/35 [==============================] - 1s 43ms/step - loss: 1.4603 - accuracy: 0.5349 - val_loss: 7.0219 - val_accuracy: 0.1252\n",
            "Epoch 132/200\n",
            "35/35 [==============================] - 1s 38ms/step - loss: 1.4531 - accuracy: 0.5376 - val_loss: 7.1554 - val_accuracy: 0.1288\n",
            "Epoch 133/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.4488 - accuracy: 0.5395 - val_loss: 7.1061 - val_accuracy: 0.1270\n",
            "Epoch 134/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.4440 - accuracy: 0.5406 - val_loss: 7.1790 - val_accuracy: 0.1314\n",
            "Epoch 135/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.4460 - accuracy: 0.5395 - val_loss: 7.2173 - val_accuracy: 0.1310\n",
            "Epoch 136/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.4407 - accuracy: 0.5401 - val_loss: 7.3668 - val_accuracy: 0.1279\n",
            "Epoch 137/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.4476 - accuracy: 0.5378 - val_loss: 7.3699 - val_accuracy: 0.1288\n",
            "Epoch 138/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.4400 - accuracy: 0.5405 - val_loss: 7.3464 - val_accuracy: 0.1301\n",
            "Epoch 139/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.4370 - accuracy: 0.5412 - val_loss: 7.4331 - val_accuracy: 0.1288\n",
            "Epoch 140/200\n",
            "35/35 [==============================] - 1s 41ms/step - loss: 1.4406 - accuracy: 0.5393 - val_loss: 7.3060 - val_accuracy: 0.1283\n",
            "Epoch 141/200\n",
            "35/35 [==============================] - 2s 44ms/step - loss: 1.4454 - accuracy: 0.5380 - val_loss: 7.5058 - val_accuracy: 0.1323\n",
            "Epoch 142/200\n",
            "35/35 [==============================] - 2s 44ms/step - loss: 1.4573 - accuracy: 0.5332 - val_loss: 7.1851 - val_accuracy: 0.1310\n",
            "Epoch 143/200\n",
            "35/35 [==============================] - 1s 38ms/step - loss: 1.5214 - accuracy: 0.5137 - val_loss: 7.5869 - val_accuracy: 0.1256\n",
            "Epoch 144/200\n",
            "35/35 [==============================] - 1s 38ms/step - loss: 1.5994 - accuracy: 0.4850 - val_loss: 6.7629 - val_accuracy: 0.1265\n",
            "Epoch 145/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 2.1115 - accuracy: 0.3656 - val_loss: 3.1909 - val_accuracy: 0.1341\n",
            "Epoch 146/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 2.2322 - accuracy: 0.3211 - val_loss: 3.5120 - val_accuracy: 0.1494\n",
            "Epoch 147/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 2.1123 - accuracy: 0.3638 - val_loss: 3.8290 - val_accuracy: 0.1494\n",
            "Epoch 148/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 2.0266 - accuracy: 0.3864 - val_loss: 3.7861 - val_accuracy: 0.1611\n",
            "Epoch 149/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.9753 - accuracy: 0.3976 - val_loss: 3.9490 - val_accuracy: 0.1534\n",
            "Epoch 150/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.9740 - accuracy: 0.3860 - val_loss: 3.5643 - val_accuracy: 0.1548\n",
            "Epoch 151/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.9436 - accuracy: 0.3883 - val_loss: 3.9611 - val_accuracy: 0.1413\n",
            "Epoch 152/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.8355 - accuracy: 0.4255 - val_loss: 4.4813 - val_accuracy: 0.1472\n",
            "Epoch 153/200\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 1.7951 - accuracy: 0.4367 - val_loss: 4.4985 - val_accuracy: 0.1440\n",
            "Epoch 154/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.7564 - accuracy: 0.4470 - val_loss: 4.9139 - val_accuracy: 0.1404\n",
            "Epoch 155/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.7394 - accuracy: 0.4519 - val_loss: 5.0237 - val_accuracy: 0.1418\n",
            "Epoch 156/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.7127 - accuracy: 0.4576 - val_loss: 5.1133 - val_accuracy: 0.1409\n",
            "Epoch 157/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.7010 - accuracy: 0.4633 - val_loss: 5.2601 - val_accuracy: 0.1427\n",
            "Epoch 158/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.6914 - accuracy: 0.4651 - val_loss: 5.4204 - val_accuracy: 0.1409\n",
            "Epoch 159/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.6804 - accuracy: 0.4679 - val_loss: 5.4134 - val_accuracy: 0.1337\n",
            "Epoch 160/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.6626 - accuracy: 0.4757 - val_loss: 5.5237 - val_accuracy: 0.1377\n",
            "Epoch 161/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.6504 - accuracy: 0.4790 - val_loss: 5.5097 - val_accuracy: 0.1341\n",
            "Epoch 162/200\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 1.6323 - accuracy: 0.4851 - val_loss: 5.8284 - val_accuracy: 0.1431\n",
            "Epoch 163/200\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 1.6117 - accuracy: 0.4911 - val_loss: 5.7402 - val_accuracy: 0.1409\n",
            "Epoch 164/200\n",
            "35/35 [==============================] - 1s 42ms/step - loss: 1.5986 - accuracy: 0.4955 - val_loss: 5.9690 - val_accuracy: 0.1373\n",
            "Epoch 165/200\n",
            "35/35 [==============================] - 1s 35ms/step - loss: 1.5817 - accuracy: 0.5007 - val_loss: 5.9619 - val_accuracy: 0.1404\n",
            "Epoch 166/200\n",
            "35/35 [==============================] - 1s 38ms/step - loss: 1.5734 - accuracy: 0.5024 - val_loss: 6.0806 - val_accuracy: 0.1355\n",
            "Epoch 167/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.5494 - accuracy: 0.5096 - val_loss: 6.0468 - val_accuracy: 0.1301\n",
            "Epoch 168/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.5364 - accuracy: 0.5145 - val_loss: 6.2519 - val_accuracy: 0.1310\n",
            "Epoch 169/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.5210 - accuracy: 0.5192 - val_loss: 6.3898 - val_accuracy: 0.1364\n",
            "Epoch 170/200\n",
            "35/35 [==============================] - 1s 37ms/step - loss: 1.5132 - accuracy: 0.5209 - val_loss: 6.4740 - val_accuracy: 0.1368\n",
            "Epoch 171/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.5045 - accuracy: 0.5228 - val_loss: 6.5550 - val_accuracy: 0.1350\n",
            "Epoch 172/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.5558 - accuracy: 0.5084 - val_loss: 6.4530 - val_accuracy: 0.1391\n",
            "Epoch 173/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.5100 - accuracy: 0.5222 - val_loss: 6.6087 - val_accuracy: 0.1377\n",
            "Epoch 174/200\n",
            "35/35 [==============================] - 2s 44ms/step - loss: 1.5022 - accuracy: 0.5234 - val_loss: 6.5441 - val_accuracy: 0.1404\n",
            "Epoch 175/200\n",
            "35/35 [==============================] - 2s 44ms/step - loss: 1.5029 - accuracy: 0.5237 - val_loss: 6.5747 - val_accuracy: 0.1364\n",
            "Epoch 176/200\n",
            "35/35 [==============================] - 2s 40ms/step - loss: 1.4965 - accuracy: 0.5269 - val_loss: 6.6419 - val_accuracy: 0.1332\n",
            "Epoch 177/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.4885 - accuracy: 0.5268 - val_loss: 6.5657 - val_accuracy: 0.1364\n",
            "Epoch 178/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.4850 - accuracy: 0.5298 - val_loss: 6.5594 - val_accuracy: 0.1314\n",
            "Epoch 179/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.5186 - accuracy: 0.5211 - val_loss: 6.2978 - val_accuracy: 0.1350\n",
            "Epoch 180/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.5927 - accuracy: 0.4987 - val_loss: 5.9463 - val_accuracy: 0.1373\n",
            "Epoch 181/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.5281 - accuracy: 0.5162 - val_loss: 6.1308 - val_accuracy: 0.1355\n",
            "Epoch 182/200\n",
            "35/35 [==============================] - 1s 38ms/step - loss: 1.4906 - accuracy: 0.5300 - val_loss: 6.5984 - val_accuracy: 0.1346\n",
            "Epoch 183/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.4827 - accuracy: 0.5319 - val_loss: 6.6042 - val_accuracy: 0.1288\n",
            "Epoch 184/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.4684 - accuracy: 0.5344 - val_loss: 6.6808 - val_accuracy: 0.1314\n",
            "Epoch 185/200\n",
            "35/35 [==============================] - 1s 43ms/step - loss: 1.4635 - accuracy: 0.5369 - val_loss: 6.8652 - val_accuracy: 0.1341\n",
            "Epoch 186/200\n",
            "35/35 [==============================] - 2s 43ms/step - loss: 1.4573 - accuracy: 0.5395 - val_loss: 6.7825 - val_accuracy: 0.1355\n",
            "Epoch 187/200\n",
            "35/35 [==============================] - 2s 44ms/step - loss: 1.4513 - accuracy: 0.5402 - val_loss: 6.9661 - val_accuracy: 0.1297\n",
            "Epoch 188/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.5170 - accuracy: 0.5229 - val_loss: 6.4969 - val_accuracy: 0.1292\n",
            "Epoch 189/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.5500 - accuracy: 0.5125 - val_loss: 6.3388 - val_accuracy: 0.1346\n",
            "Epoch 190/200\n",
            "35/35 [==============================] - 1s 38ms/step - loss: 1.5106 - accuracy: 0.5237 - val_loss: 6.4513 - val_accuracy: 0.1319\n",
            "Epoch 191/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.4730 - accuracy: 0.5345 - val_loss: 6.6657 - val_accuracy: 0.1323\n",
            "Epoch 192/200\n",
            "35/35 [==============================] - 1s 32ms/step - loss: 1.4612 - accuracy: 0.5383 - val_loss: 6.7793 - val_accuracy: 0.1323\n",
            "Epoch 193/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.4471 - accuracy: 0.5411 - val_loss: 6.8201 - val_accuracy: 0.1337\n",
            "Epoch 194/200\n",
            "35/35 [==============================] - 1s 43ms/step - loss: 1.4578 - accuracy: 0.5392 - val_loss: 6.7078 - val_accuracy: 0.1404\n",
            "Epoch 195/200\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 1.4518 - accuracy: 0.5405 - val_loss: 7.0290 - val_accuracy: 0.1328\n",
            "Epoch 196/200\n",
            "35/35 [==============================] - 1s 36ms/step - loss: 1.4466 - accuracy: 0.5417 - val_loss: 6.9105 - val_accuracy: 0.1332\n",
            "Epoch 197/200\n",
            "35/35 [==============================] - 1s 36ms/step - loss: 1.4437 - accuracy: 0.5423 - val_loss: 6.9579 - val_accuracy: 0.1359\n",
            "Epoch 198/200\n",
            "35/35 [==============================] - 2s 45ms/step - loss: 1.5301 - accuracy: 0.5201 - val_loss: 6.3603 - val_accuracy: 0.1337\n",
            "Epoch 199/200\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 1.5466 - accuracy: 0.5114 - val_loss: 6.4563 - val_accuracy: 0.1373\n",
            "Epoch 200/200\n",
            "35/35 [==============================] - 1s 33ms/step - loss: 1.5065 - accuracy: 0.5234 - val_loss: 6.2759 - val_accuracy: 0.1328\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7af5ac26a980>"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkpoint callback\n",
        "checkpoint_path = \"/content/drive/MyDrive/Model_data/model_checkpoint_personality_pred.h5\"\n",
        "checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n"
      ],
      "metadata": {
        "id": "e255vJG-UAqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_aug_pad, y_train_aug_cat, epochs=2, batch_size=512, validation_data=(X_test_pad, y_test_cat), callbacks=[checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3IyPCPLVsE4",
        "outputId": "433f4744-7d48-462e-bfa7-19030ee0a200"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "35/35 [==============================] - ETA: 0s - loss: 1.4868 - accuracy: 0.5287\n",
            "Epoch 1: val_loss improved from inf to 6.62030, saving model to /content/drive/MyDrive/Model_data/model_checkpoint_personality_pred.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r35/35 [==============================] - 7s 204ms/step - loss: 1.4868 - accuracy: 0.5287 - val_loss: 6.6203 - val_accuracy: 0.1377\n",
            "Epoch 2/2\n",
            "35/35 [==============================] - ETA: 0s - loss: 1.4630 - accuracy: 0.5366\n",
            "Epoch 2: val_loss did not improve from 6.62030\n",
            "35/35 [==============================] - 1s 34ms/step - loss: 1.4630 - accuracy: 0.5366 - val_loss: 6.7086 - val_accuracy: 0.1301\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7af5ac298eb0>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_dataset.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "H1RYNP_Orb62",
        "outputId": "e512e022-59c9-4900-e630-30ab5979e5f6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                TEXT  MBTI\n",
              "0  httpwwwyoutubecomwatchvqsxhcwekrwhttpmediatumb...  INFJ\n",
              "1  im finding lack post alarmingsex boring positi...  ENTP\n",
              "2  good one _____ httpswwwyoutubecomwatchvfhigbol...  INTP\n",
              "3  dear intp enjoyed conversation day esoteric ga...  INTJ\n",
              "4  youre firedthats another silly misconception a...  ENTJ"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-812cd069-6cd4-432d-87d2-f0d69a6dbfd9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TEXT</th>\n",
              "      <th>MBTI</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>httpwwwyoutubecomwatchvqsxhcwekrwhttpmediatumb...</td>\n",
              "      <td>INFJ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>im finding lack post alarmingsex boring positi...</td>\n",
              "      <td>ENTP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>good one _____ httpswwwyoutubecomwatchvfhigbol...</td>\n",
              "      <td>INTP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>dear intp enjoyed conversation day esoteric ga...</td>\n",
              "      <td>INTJ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>youre firedthats another silly misconception a...</td>\n",
              "      <td>ENTJ</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-812cd069-6cd4-432d-87d2-f0d69a6dbfd9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-812cd069-6cd4-432d-87d2-f0d69a6dbfd9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-812cd069-6cd4-432d-87d2-f0d69a6dbfd9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-753492bc-69ce-4280-9f9a-14d2515a9ab8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-753492bc-69ce-4280-9f9a-14d2515a9ab8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-753492bc-69ce-4280-9f9a-14d2515a9ab8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "full_dataset",
              "summary": "{\n  \"name\": \"full_dataset\",\n  \"rows\": 11143,\n  \"fields\": [\n    {\n      \"column\": \"TEXT\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11143,\n        \"samples\": [\n          \"much happyfunny thing growing finally get whatever want except anything might actually want tonguei guess im kind social darwinist think powerful win long run law human society always always theresits almost people personality typenervous date make confident relaxed writing long list word make feel way repeating overthere two kind boredom type much pointless type little pointless ive transitioned type thestop paying video game ever heard postpurchase bias like exact opposite buyer regret buy something expensive try justify purchase one way tomaybe thats bad thing long drop activity cuz day wouldnt call productivity issueholy shit shockedyes im often looking thought head move head around lot make weird face im thinking really hard often smiling like madman staring intensely thing please tell realhe life reasonably ever afterbmineral probably life grandma nowyou aware lydia death ruled suicide daughter think abandoned worst way possiblei think absurd say skylers nagging made walt coocoo cocoa puffsgretchen eliother subsequent action suggest merely concerned family safety shes really afraid someone family could end like gu mean someone like gusi take opposite view think shes pretty much exactly walt main difference cant cook meth much lose walt doesyeah way make hypocrite throughout dont think even say walt wrong certainly think putting family danger butim defending action im showing hypocritical yes helped launder money disagree walts meth cooking moral concern thattrue whole gu thing finding dangerous really perspective walt simply something illegal like beneke thatshe never aided told wrong time accomplice attempting suicide get kid away walt accomplice shebut thats normal people reason hate herit like httpwwwthreadbombingcomdatamediabbgif except intense every five minutesthank dear sigs proudneutral evil neutral evil villain whatever get away pure simple shed tear kill whether profit sport orits default method learning pretty much everything know learned dont understand people lack motivation ability natural ashello kittehwhat dany need one httpsakuranokuniweeblycomuploads_origjpgshed much better spear shortsword anything else considering shell probably riding dragon bow would useless close quarter superfluous dragonback amhysadamn really cant wait new seasonlittlefinger god know game littlefinger playing lord varysno yes yesfunny top three favorite character littlefinger top cant decide whether like daenerys tyrion dont understand hate littlefinger getsroose bolton idiot think tywin going let stay lord winterfell im assuming tywin promised exchange robbs life know sansa iswhat hell one who actually rooting team lannister face robb stark simply impulsive king one simply balk oath made one powerful lordreminds hamletreallifecomthen youll fucking bride wooden cock say say best moment seasonlittlefingers speech nuff saidlike oh yeah dont know anybody else caught seems tywins decision marry sansa tyrion implies plan kill robb stark capture also bran bortherit could writer fucked also possible north didnt send literally every last soldier good portion must stayed behind holdingwhat hell mormont didnt die othis also incompetence fbi swat doesnt surprise anywhere near much well joes plan worked considering people bunch lunatic nothing atagreed ended season episode joe joey meet house would epic cliff hanger next seasoni agree bitch mean favorite characterdid see crime nature pulled last night fucking mad angry angry angry angry angry angryim getting hyperlink text think something firefox since occur every site despite fact adblocker\",\n          \"option applies dont toilet paper holding thingie toilet paper roll valve next toilet proudtheres hidden theme crazy httpsdldropboxcomufunnynietzscheshavingjpg httpsdldropboxcomufunnyjuicejpg source hipsterhitlercom fennel soup beware us metric system tongue onion bulb fennel litre vegetable stock g butter g sour cream either proven\\u00e3al herb thyme caraway rosemary basilthere might correlation enneagram type scale strong theory enfp usually high p andor n lower f entps enfps occupy type onyou need enfp course dont expect himher change time soon set deadline expect give respite otherwise good luck patienceenfps classic jack trade quick learn new skill usually great depth let see random skill fart hand neck knee last twononconformity definately rebel let explain rebellion fighting status quo authority enfps dont fight authority status quo agreeyes realize evil people meet someone first time initially assume theyre fundamentally good person thats kind credit givemy result test ability score strength dexterity constitution intelligence wisdom charisma using dd class estp barbarian chaotic evil neutral theyre wild untamed powerful rely strength brain istp rogue chaotic evil neutral cunning freewhat thread confusedshy extravert outgoing introvert may alike aspect differ greatly others shy extravert feel spend much alone time example day outoh alright mixed dont agree view intuitively good grasp talking use word god might able express definitionpure agnostic enfp apatheismindifference religion strong agnosticism know god exists weak agnosticism dont know god exists interested inhow falsify law nature let take newtonian gravity accept viewed law nature past present apple normal condition fallenfp chaotic good hereim enfp need make poll type p im well aware topictonguegeography music sadly music teacher werent good usually oh computer science fun proudread didnt say explore information information starting point exploration world meant interpreted figuratively let explain ni ne ni deal conditional future information different way ne go current point several different futureconditional point whatas title post gon na short tongue se see si see ne see could ni see thats wink discus proudhttpjwalkblogcomimageshandmusicjpg represents hand farting show immoralityon passiveaggressive discussion keirsey noticed similar thing distinguished informative directive role communication directive type sts nj takingas said group based keirsey distinguishing group keirsey look human behaviour view dichotomy communication say action weenfp musician cool im guitarist ibanez guitar electric sa acoustic ewc also owned another electric sold cheaply really bad onei forgot main topic crazy extraverted perceiver switch isnt big esspecially go original jungian system primary function balanced could also question test accuracy often feeling function test question ne ni somewhat similar fi fe ti te etc could mean yourehttpuploadwikimediaorgwikipediacommonsthumbmandel_zoom__satellite_julia_islandjpgpxmandel_zoom__satellite_julia_islandjpg show soulhttppersonalitycafecomattachmentphpattachmentidstcd show noisehttpblstbmsncomidcbdcbfabcbcfffjpg show opposite pictureo correlation would quadras socionics novery good analysis antagonist think explaining third ring havent read term book could help explaining distinction chose point directive v informativemath easy atleast school better nt around crazy like calculus algebra dont really like geometry guess im much visualmy best friend since first grade flatmate infj get eachother well people say quarrel like old couple dunno thats positive thing say friendsno adhd doubt adhd either dunno adhd people often problem concentration im easily distracted get distressed annoyed butthis httpwwwyoutubecomwatchvwkp_namsrmm want like im old note anyone meetup year laughingradiohead sigur ro fleet fox joanna newsom sufjan stevens explosion sky talk talk make say think beatles yann thiersen mgmt manymorebuttoolazytocontinue p saidbarney enfp ralph infp someone dislike nfps seems tonguehttpinfobeautifulsamazonawscomintnoones__jpg show riddlelooks like pacman running awayemt infj alternatively enfj immense drive selfdevelopment discovery programmed doctor need fe deal patient side would due program thinkspongebob definately e p there nothing he really structured go flow defined ft sn cook good get intowelcome fencesitter club xnfps even dont like sitting tongue tend get distracted easily even thing take interest consider ato intellectual also aspect disconcern success andor career advancement like intellectual pursuit done knowledge fun mean end thatsenfp green eye sake proof tongue httppersonalitycafecommembersarachnophobiaalbumsmepictureshinyeyepng appear rather dark pretty greyso lately ive run thought appealed clear problem thinking intro extraversion iescale single missinterpreted scalebe boring dont give horrible time time let get vibe there nothing le exciting world nothing better get rid enfps boredomdefinately spring great able sit outside sun without dieing dehydration also like whole vibe spring nature awakening compared autumn wheni made two new one httppersonalitycafecommembersarachnophobiaalbumsstuffpictureenfpmotjpg\",\n          \"ive driven offroad since soi grew sierra nevada mountain went wood cutting every year fire wood winter fond memory day sowaterid want keep water sure mean key component much beautywaterfalls mist rain splash drip mud puddle lake river rain cloud snow ice rainhintstired chaos headachetoo much work never drunk believe meyoure missing anything cant think anything ive ever done drunk wouldnt fun slightly buzzed soberbut ii really isolate really dont want around people underlying reason could anything mad world stress much contact orrofl look like pig character one kid cartoon farm animal dont remember name though ill prolly remember name completely random time ini hate war people cant seem figure make peace without fighting itso fight kinda seems stupid gladly fight protect people lovethoughlife box chocolatesits like flea market time find exactly youre looking time look around ask wtf dont spend money youhate familylol child didnt even need hide forgotten two year old left ice skating rink parent didnt even notice wasnt got waydepends mostly im shopping im tall slender lot place dont clothes fit well jean usually wrangler walmart p darker mood go dork work lol p st rd th youngest born nd th oldest first one like really gon na freakinheres couple pic dug archive p best friend yosemite number another friend hiking near halfmother esfj father entp older brother istp younger brother esfp sister enfjok promised dug old high school art portfolio two watercolor painted got little hammered yearsmyi lay camera rest today one kid dont know one broke itit sad day bright side cleaning cabin found old art portfolio dont greatest handwriting work p sorry large pic dont program installed moment resize pictureguess need download andi think understand youre trying say imagination ambition without selling soul make culture ambition blind consequence iswait huh sowhat saying youre nice goodi let anger simmer wait till explode someone desperately deserves itor vent closest friend occasionally ill go blow steam gym golmao awesomethe best way get know infp experience p casual hang truly get know appreciate make feel special andthe biggest problem see pairing question thing become personal infps enfps general prone take thing personallytheyre also prone beno dont think cheating agree others youre masturbating instead engaging sex willing partner youre creating problem perhaps shouldthe day ex wife told slept friend mine betrayal two people enormity still nearly crush think even dayand nearlyi love travel havent anywhere near many place id like go though far mexico korea stopped japan layover visited state thei always loved exploration new idea especially deal flight andor space flight love science theoretical science guess could say ive always farpandora solo piano radioand playing opus dustin ohalloran armyinstructor moment teach work radio computer enjoy relatively nonrepetitive something couldnt stand working factory thehave take mbti test better yet take several youll know sure he intp infp tell little hell intrigued enoughthe advice offer experience dont get caught trap thinking delegate help keep sanity anyfor reason talk people look start talkingsomehow manage acknowledge right moment keep talkingand keep listening sometimesi know sure im right head p often ill lost thought think something funny start laughing blueeveryone earshot start looking ati total daydreamer child parent fight school keep holding back year nd gradebecause daydreamed much mom tell allalmost enfjs know cool hell really dont much negative say type individual different story make difficult thing toabsolutely dont know gon na happen view though since wont know day hour live good life take one day ahttpwwwyoutubecomwatchvtufojvopqi got black wool coat cocktail flavored jelly belly merry christmas merry christmas allyes good yes desire math yes science yes depending circumstance nothing supposed week two absolutely nothing isi say time dont post someone el wall dont want bother dont visit everyones profile dont want look like stalkerhey im board wall plenty room dont bite eitherwell hard anyway pim looking forward another year watching kid grow sending son first day school ill proud getting debt getting car fixedori two boy michael meric dont think im going child girl name melinda kaitlin kaileigh boy name eliot collinsorry took long reply tdy training air bit p view position one able help lot others stand whats right toin experience living world aggressiveness leadership skill demanded military ive found turning emotionsensitivity side highly counter productive andvery well put experience well\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MBTI\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          \"INFJ\",\n          \"ENTP\",\n          \"ENFJ\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dpyVKIN3rdkY"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQTRzq77soQf",
        "outputId": "080e592f-ac90-4a51-a059-b2e810b575ad"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 44ms/step\n",
            "The predicted MBTI personality type for the provided text is: INFP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "mu-e2_i8s5S9"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_aug_pad, y_train_aug_cat, epochs=2, batch_size=64, validation_data=(X_test_pad, y_test_cat), callbacks=[checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugavwFfDviGS",
        "outputId": "834bddad-14fb-41e8-c580-0d786c34b982"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "279/279 [==============================] - ETA: 0s - loss: 3.0408 - accuracy: 0.1664\n",
            "Epoch 1: val_loss improved from inf to 2.49538, saving model to /content/drive/MyDrive/Model_data/model_checkpoint_personality_pred.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r279/279 [==============================] - 53s 159ms/step - loss: 3.0408 - accuracy: 0.1664 - val_loss: 2.4954 - val_accuracy: 0.1853\n",
            "Epoch 2/2\n",
            "279/279 [==============================] - ETA: 0s - loss: 2.4971 - accuracy: 0.1751\n",
            "Epoch 2: val_loss improved from 2.49538 to 2.40755, saving model to /content/drive/MyDrive/Model_data/model_checkpoint_personality_pred.h5\n",
            "279/279 [==============================] - 32s 116ms/step - loss: 2.4971 - accuracy: 0.1751 - val_loss: 2.4076 - val_accuracy: 0.1920\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fe866833220>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model_checkpoint = \"/content/drive/MyDrive/Model_data/model_checkpoint_personality_pred.h5\"\n",
        "\n",
        "model = load_model(model_checkpoint)"
      ],
      "metadata": {
        "id": "z6hhR1hJwRr5"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def predict_personality(text):\n",
        "  preprocessed_text = preprocess_text(text)\n",
        "\n",
        "\n",
        "  sequence = tokenizer.texts_to_sequences([preprocessed_text])\n",
        "\n",
        "  padded_sequence = pad_sequences(sequence, maxlen = 100, padding='post')\n",
        "  prediction = model.predict(padded_sequence)\n",
        "  predicted_class_index = np.argmax(prediction)\n",
        "\n",
        "  predicted_class = label_encoder.inverse_transform([predicted_class_index])\n",
        "\n",
        "  return predicted_class[0]\n"
      ],
      "metadata": {
        "id": "zwNRUPR1yZVN"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"Radom people suck. Happy birthday to you. Good luck to you and have a great day\"\n",
        "predicted_mbti = predict_personality(sample_text)\n",
        "print(f\"The predicted MBTI personality type for the provided text is: {predicted_mbti}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KB2vGMqtybPt",
        "outputId": "4d49c626-b15c-48a7-ca5e-598b8e0b1f7f"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 717ms/step\n",
            "The predicted MBTI personality type for the provided text is: INFP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SkjZissLyfmt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}